{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89522b97-3f30-45fb-b631-ac4ba7c1ad77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import wordninja\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, default_data_collator, get_scheduler\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab319d7e-a5e6-495b-906e-c5c511b2cc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/home/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a58cb4d-7fee-460e-8757-37ce1e980f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#論文中の記述(4ページ), \"tokenizer based on English Wikipedia unigram frequencies\"\n",
    "#Huggingfaceのトークナイザで上記に該当するの\n",
    "    #bert-base-uncased**\n",
    "    #roberta-base\n",
    "    #distilbert-base-uncased\n",
    "    #distilbert-base-multilingual-cased\n",
    "#**実験に利用\n",
    "#unigram freqencies?tokenizerのアルゴリズムは結局何なんだろう? Unigram or WordPiece?\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    \"\"\"\n",
    "    A function that tokenizes all logs\n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"log\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65a746-d677-4f3f-bc83-b643c3c7547a",
   "metadata": {},
   "source": [
    "# ここではログデータのロードと前処理, そしてhuggingfaceデータセットへの変換を行った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f2f0abe-8522-40fb-94f7-d8a14d32d0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log2list(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd49275e-d4a2-433e-b179-76cf2d8b58ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId    Label\n",
       "0  blk_-1608999687919862906   Normal\n",
       "1   blk_7503483334202473044   Normal\n",
       "2  blk_-3544583377289625738  Anomaly\n",
       "3  blk_-9073992586687739851   Normal\n",
       "4   blk_7854771516489510256   Normal"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hdfs_anomaly = pd.read_csv(\"./logdata/hdfs/anomaly_label.csv\")\n",
    "df_hdfs_anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e14a948b-9601-49a7-bd1b-e858d2b1aa34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blk_-66330728533676520'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1939a70-3e22-4aed-ba08-c412b02a05e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'log', 'BlockId', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hdfs_logs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "677a40f4-fe8f-46e9-aeb1-a98a762bb37f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hdfs_logs = pd.read_csv(\"./logdata/hdfs/hdfs_log_label_pairs.csv\", usecols=['log', 'BlockId', 'Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78880523-04c2-4ba8-92cf-1b9692ca3d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hdfs_logs[\"label\"] = df_hdfs_logs[\"Label\"].map(lambda x : 0 if x==\"Normal\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "745fe509-4b54-483c-85e6-8bbabff88881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109 203518 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109 203519 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$PacketResp...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  081109 203518 143 INFO dfs.DataNode$DataXceive...   \n",
       "1  081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "2  081109 203519 143 INFO dfs.DataNode$DataXceive...   \n",
       "3  081109 203519 145 INFO dfs.DataNode$DataXceive...   \n",
       "4  081109 203519 145 INFO dfs.DataNode$PacketResp...   \n",
       "\n",
       "                    BlockId   Label  label  \n",
       "0  blk_-1608999687919862906  Normal      0  \n",
       "1  blk_-1608999687919862906  Normal      0  \n",
       "2  blk_-1608999687919862906  Normal      0  \n",
       "3  blk_-1608999687919862906  Normal      0  \n",
       "4  blk_-1608999687919862906  Normal      0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hdfs_logs = df_hdfs_logs.rename(columns={\"log\":\"text\"})\n",
    "df_hdfs_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91c3c1e8-7c4c-4958-91a6-0db8ddb6329f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_log(text, remove_digits=True):\n",
    "    text = text.replace('[', \" \")\n",
    "    text = text.replace(\".\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\":\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\";\", \" \")\n",
    "    text = text.replace(\"=\", \" \")\n",
    "    text = text.replace(\"*\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.lower()\n",
    "    text = \" \".join(wordninja.split(text))\n",
    "    remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "    if remove_digits:\n",
    "        return remove_num(text)\n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7c0ec90-de35-4758-800b-0ae41950f9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_hdfs_logs = pd.DataFrame({\"log\" : hdfs})\n",
    "# df_hdfs_logs[\"BlockId\"] = df_hdfs_logs[\"log\"].map(lambda line : re.search(r\"blk_[-]*[0-9]+\", line).group(0))\n",
    "# df_hdfs = df_hdfs_logs.merge(df_hdfs_anomaly, how=\"inner\", on=[\"BlockId\"])\n",
    "# df_hdfs.to_csv(\"./logdata/hdfs/hdfs_log_label_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb14e999-3838-4a6c-bb97-2bcdea2ea79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hdfs = log2list(\"./logdata/hdfs/HDFS.log\")\n",
    "# log_label_pairs = {}\n",
    "# pbar = tqdm(hdfs)\n",
    "# for line in hdfs:\n",
    "#     pbar.update(1)\n",
    "#     m = re.search(r\"blk_[-]*[0-9]+\", line).group(0)\n",
    "#     blkid = m.group(0)\n",
    "#     log_label_pairs[line] = df_hdfs_anomaly[df_hdfs_anomaly[\"BlockId\"] == blkid][\"Label\"]\n",
    "# df_hdfs_logs = pd.DataFrame({\"log\" : hdfs})\n",
    "# df_hdfs_logs[\"BlockId\"] = df_hdfs_logs[\"log\"].map(lambda line : re.search(r\"blk_[-]*[0-9]+\", line).group(0))\n",
    "# df_hdfs = df_hdfs_logs.merge(df_hdfs_anomaly, how=\"inner\", on=[\"BlockId\"])\n",
    "# df_hdfs.to_csv(\"./logdata/hdfs/hdfs_log_label_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e021cbd4-edb9-4706-8f41-6c840217006e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ログデータのロード\n",
    "hdfs = log2list(\"./logdata/hdfs/HDFS.log\")\n",
    "bgl = log2list(\"./logdata/bgl/BGL.log\")\n",
    "hpc = log2list(\"./logdata/hpc/HPC.log\")\n",
    "zookeeper = log2list(\"./logdata/zookeeper/Zookeeper.log\")\n",
    "proxifier = log2list(\"./logdata/proxifer/Proxifier.log\")\n",
    "\n",
    "hadoop = []\n",
    "for path in glob.glob(\"./logdata/hadoop/**/*.log\"):\n",
    "    hadoop += log2list(path)\n",
    "len(hadoop)\n",
    "\n",
    "#NOTE\n",
    "#Switchのみちょっとどうやって特徴テンプレートに埋め込んだのか分からなかったので,元論文よりデータ少なめです\n",
    "switch = []\n",
    "for path in glob.glob(\"./logdata/switch/**/template.txt\"):\n",
    "    switch += log2list(path)\n",
    "len(switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f728f659-1d4d-4bf6-95ff-3eab6127d372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS: 11175629\n",
      "BGL 4747963\n",
      "HPC 433490\n",
      "Zookeeper 74380\n",
      "Proxifier 21329\n",
      "Switch 959\n",
      "Hadoop 394310\n"
     ]
    }
   ],
   "source": [
    "print(\"HDFS:\", len(hdfs))\n",
    "print(\"BGL\", len(bgl))\n",
    "print(\"HPC\", len(hpc))\n",
    "print(\"Zookeeper\", len(zookeeper))\n",
    "print(\"Proxifier\", len(proxifier))\n",
    "print(\"Switch\", len(switch))\n",
    "print(\"Hadoop\", len(hadoop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2f87ad8-a584-4d01-a066-889eb47df7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = hdfs + bgl + hpc + zookeeper + proxifier + switch + hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5725bf54-73b0-466e-bb28-77922a6b6963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ff789e53-9891-4b18-82e3-61d46cd17630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30be9f8d-93ed-4878-9d7d-0dca0d669f76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- 1121495637 2005.07.15 R20-M0-NB-C:J08-U11 2005-07-15-23.33.57.541740 R20-M0-NB-C:J08-U11 RAS KERNEL INFO generating core.28730'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718adf9a-c9ad-454f-b93a-4f7e8fba32fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = []\n",
    "#NOTE\n",
    "#ここで記号を区切ってwordninjaで塊除去を行っている\n",
    "#なお, かなり時間がかかるので実行には注意が必要\n",
    "#もう少し賢く書きたかったが, その書き方を思いつく時間とナイーブに実行する時間はどうせ同じぐらいだろう\n",
    "# for i, text in enumerate(tqdm(data)):\n",
    "#     text = text.replace('[', \" \")\n",
    "#     text = text.replace(\".\", \" \")\n",
    "#     text = text.replace(\",\", \" \")\n",
    "#     text = text.replace(\":\", \" \")\n",
    "#     text = text.replace(\"/\", \" \")\n",
    "#     text = text.replace(\";\", \" \")\n",
    "#     text = text.replace(\"=\", \" \")\n",
    "#     text = text.replace(\"*\", \" \")\n",
    "#     text = text.replace(\"_\", \" \")\n",
    "#     text = text.replace(\"-\", \" \")\n",
    "#     text = text.lower()\n",
    "#     target_data[i] = \" \".join(wordninja.split(text))\n",
    "# target_array = np.array(target_data)\n",
    "# df_log = pd.DataFrame(target_array, columns=[\"log\"])\n",
    "# df_log = df_log[df_log.log != \"\"]\n",
    "# df_log.to_csv(\"./logdata/all_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c290f0-4819-48f7-bbc5-fcfe7bd63d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8f77ce5f-00d2-43b8-a4ce-2807ed5f31ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1121495637 2005 07 15 r 20 m 0 nb c j 08 u 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081111 023358 19 info df s fs data set deletin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081110 213754 33 info df s fs name system bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081110 230147 16884 info df s data node data x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1131981710 2005 11 14 r 44 m 0 nb c j 09 u 01 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604700</th>\n",
       "      <td>081111 075637 28 info df s fs name system bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604701</th>\n",
       "      <td>081110 022458 13 info df s data block scanner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604702</th>\n",
       "      <td>081110 210159 28 info df s fs name system bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604703</th>\n",
       "      <td>081111 055917 21209 info df s data node packet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16604704</th>\n",
       "      <td>081110 015904 6039 info df s data node packet ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16604704 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        log\n",
       "1         1121495637 2005 07 15 r 20 m 0 nb c j 08 u 11 ...\n",
       "2         081111 023358 19 info df s fs data set deletin...\n",
       "3         081110 213754 33 info df s fs name system bloc...\n",
       "4         081110 230147 16884 info df s data node data x...\n",
       "5         1131981710 2005 11 14 r 44 m 0 nb c j 09 u 01 ...\n",
       "...                                                     ...\n",
       "16604700  081111 075637 28 info df s fs name system bloc...\n",
       "16604701  081110 022458 13 info df s data block scanner ...\n",
       "16604702  081110 210159 28 info df s fs name system bloc...\n",
       "16604703  081111 055917 21209 info df s data node packet...\n",
       "16604704  081110 015904 6039 info df s data node packet ...\n",
       "\n",
       "[16604704 rows x 1 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f6231289-bfdf-458b-8390-fdb54e64762f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #訓練データとテストデータに分ける\n",
    "# TRAIN_TEST_SPLIT_RATE = 0.8\n",
    "# df_log[\"is_train\"] = np.array([np.random.rand() < TRAIN_TEST_SPLIT_RATE for _ in range(len(df_log))])\n",
    "# df_log[~df_log.is_train].to_csv(\"./logdata/test_logs.csv\")\n",
    "# df_log[df_log.is_train].to_csv(\"./logdata/train_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a042a-1c80-45b6-bb9f-1764c3f698f0",
   "metadata": {},
   "source": [
    "# データセットのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84797a5b-2cd6-4873-98ef-db9332f0b2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/csv/default-a5653168cf0fdeb7/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files = \"./logdata/all_logs.csv\", cache_dir=\"/home/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c57e4d-fede-4047-8294-0cf24ae84082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c364de93-15b7-4b7f-887d-cf36593733c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shuffled_dataset = dataset[\"train\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4da23b3-6091-4f59-9437-e403ddddba64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#とりあえず変な記号などは消えた\n",
    "# shuffled_dataset['log'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e487cdf5-d339-4e79-bf5b-bbe138baa020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#データセットのトークナイズ\n",
    "# tokenized_datasets = shuffled_train.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b2c8cf-bb99-4de1-ba1f-501b50fd84ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in tqdm(range(0, len(train_dataset), 1000)):\n",
    "        yield train_dataset[i : i + 1000][\"log\"]\n",
    "        \n",
    "def get_training_corpus_without_numbers():\n",
    "    for i in tqdm(range(0, len(train_dataset), 1000)):\n",
    "        yield train_dataset[i : i + 1000][\"log_without_numbers\"]\n",
    "        \n",
    "def get_training_corpus_small():\n",
    "    for i in tqdm(range(0, len(train_dataset_small_samples), 1000)):\n",
    "        yield train_dataset_small_samples[i : i + 1000][\"log\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb12445c-81a0-45ae-9981-e1f33259f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 3165/13284 [00:22<01:10, 143.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100031984 7.530395114697544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 0\n",
    "num_samples = 0\n",
    "MILLION = 10**6\n",
    "THRESHOLD = 100 * MILLION\n",
    "for batch in get_training_corpus():\n",
    "    num_tokens += sum([len(line.split(\" \")) for line in batch])\n",
    "    num_samples += 1000\n",
    "    if num_tokens > THRESHOLD:\n",
    "        break\n",
    "ave_tokens = num_tokens / len(train_dataset)\n",
    "print(num_tokens, ave_tokens)\n",
    "LIMIT = num_samples\n",
    "shuffled_dataset = train_dataset.shuffle(seed=42)\n",
    "train_dataset_small_samples = shuffled_dataset.select(range(LIMIT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d58bc-4e6e-4c0a-a3ca-0d08f30a3ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# オリジナルのトークナイザからの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c580c9ee-8a30-4579-9e65-fb6574fd9ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "# pre_tokenizer.pre_tokenize_str(train_dataset[1000][\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b2fa293-0a15-402d-8b20-84817d739e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "# special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "# trainer = trainers.WordPieceTrainer(\n",
    "#     vocab_size=5000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    "# )\n",
    "# tokenizer.train_from_iterator(get_training_corpus_small(), trainer=trainer)\n",
    "# tokenizer.save(f\"./log_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f904504-ede1-4e23-a5b8-d0276a09d22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding = tokenizer.encode(train_dataset[100][\"log\"])\n",
    "# print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ec888-6698-43d2-a376-31b41cbb9b16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERTトークナイザからの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52961da0-f672-4cd9-ad72-8a39ddf728d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# old_tokenizer(train_dataset[1000][\"log\"])\n",
    "# new_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus_without_numbers() , 25000)\n",
    "# new_tokenizer.save_pretrained(f\"./log_tokenizer_from_old_without_numbers_large\")\n",
    "# new_tokenizer(train_dataset[1000][\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32f37fb-3a3b-47dc-b079-83cbb1db5977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 180\n",
    "log_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_large/\", model_max_length=MODEL_MAX_LENGTH)\n",
    "log_tokenizer_w_n = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_without_numbers/\", model_max_length=MODEL_MAX_LENGTH, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66edba5-3dd6-4b38-83a0-2b6189ef3721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '081110', '2149', '##34', '1553', '##8', 'info', 'df', 's', 'data', 'node', 'packet', 'responder', 'packet', 'responder', '2', 'for', 'block', 'b', 'lk', '7019', '##2130', '##7309', '##51', '##8527', '##0', 'terminating', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = log_tokenizer(train_dataset[999][\"log\"])\n",
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a61486-9a4d-45c3-bc2b-24d472964aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'info', 'df', 's', 'data', 'node', 'packet', 'responder', 'packet', 'responder', 'for', 'block', 'b', 'lk', 'terminating', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = log_tokenizer_w_n(train_dataset[999][\"log_without_numbers\"])\n",
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eebcdc-dbce-4c0a-ae86-e8a02b60b1b5",
   "metadata": {},
   "source": [
    "# ログから数字除去したコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74485a88-8a9c-43aa-8999-88eec6694214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_numerical(example):\n",
    "#     remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "#     example[\"log\"] = [remove_num(eg) for eg in example[\"log\"]]\n",
    "#     return example\n",
    "\n",
    "# dataset_without_numbers = dataset.map(remove_numerical, batched=True)\n",
    "# train_log_without_numbers = train_dataset_without_numbers.to_pandas()\n",
    "# train_log_without_numbers.set_index(train_log_without_numbers[\"Unnamed: 0\"])\n",
    "# train_log_without_numbers = train_log_without_numbers.iloc[:, 1:]\n",
    "# ds_without_numbers = dataset[\"train\"].add_column(\"log_without_numbers\", dataset_without_numbers[\"train\"][\"log\"])\n",
    "# df_without_numbers = ds_without_numbers.to_pandas()\n",
    "# df_without_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bfb8d7-f071-4383-9afb-fbaf0a401e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データをトークナイズした箇所\n",
    "# 時間かかるから保存済みなので, あまり実行しないで良いかな\n",
    "def tokenize_function(examples):\n",
    "    result = log_tokenizer_w_n(examples[\"log_without_numbers\"])\n",
    "    if log_tokenizer_w_n.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"log\", 'Unnamed: 0', 'Unnamed: 0.1', 'log_without_numbers']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d4840d-2e9a-4f17-ac58-30f1ab3f8cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "765afcb3-8b96-47d9-9aee-700815f6a956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Log 0 length: 16'\n",
      "'>>> Log 1 length: 40'\n",
      "'>>> Log 2 length: 25'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[:3]\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Log {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c8fa64be-d86e-477c-84f6-37e610b54796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Unnamed: 0', 'Unnamed: 0.1', 'log_without_numbers', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eda2a65f-528c-48b9-badf-ccd7ae9dcb61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated log length: 81'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in \n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated log length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c63b356-a7a2-49f8-81b9-796fd7f8c8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 81'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ff9d73a8-9bd9-4310-9fa7-37891d90567f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c66082-0c36-4794-ba8b-2e79e6bb65fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_labels_column(examples):\n",
    "    result = examples\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1341527b-fb5f-437b-a95a-1a3be1356977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 1519807\n",
       "})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一時間かかるからなるべくやらないで\n",
    "# lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "# lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bd45fd87-d1de-478a-9b17-e90b4a5872d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2token = {v:k for k, v in log_tokenizer_w_n.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9c1d228c-1dd3-47d7-8b3b-efc26fad4745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'info df s fs name system block name system delete b lk is added to invalid set of [SEP] [CLS] info df s data node packet responder received block b lk of size from [SEP] [CLS] info rm communicator al locator org apache had oop map reduce v app rm rm container al locator re calculating schedule headroom memory v cores [SEP] [CLS] info df s fs name system block name system delete b lk is added to invalid set of [SEP] [CLS] info df s data node packet responder received block b lk of size from [SEP] [CLS] r m nf c j u r m nf c j u ras kernel info iar dear c c [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir b lk [SEP] [CLS] r m nf c j u r m nf c j u ras kernel info iar dear bec [SEP] [CLS] null null ras m mcs error ido proxy db hit assert condition assert expression source'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tokenizer_w_n.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f4b863-8040-4488-b25b-f9fd64436f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_datasets = load_from_disk(\"./logdata/logdata_for_pretraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f7ee8b-3e00-4407-9299-b4fefaba7204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] info df s data node packet responder received block b lk of size from [SEP] [CLS] r m n c [MASK] u [MASK] m n c [MASK] u ras kernel info microseconds spent in the [MASK] s signal handler during calls microseconds was the [MASK] time [MASK] a single instance [MASK] a correctable [MASK] [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir [MASK] lk [SEP] [CLS] info df s fs name system block name system add stored block block map updated is added [MASK] b [MASK] [MASK] [SEP] [CLS] [MASK] df s data node [MASK] x ce iver receiving block b lk s rc [MASK] st [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir b lk [SEP] [CLS] r m ne c j u r m ne c j u ras kernel info torus receiver z input pipe error s d cr x f detected and corrected [SEP] [CLS]'\n",
      "\n",
      "'>>> info df s fs name system block [MASK] [MASK] delete [MASK] lk is added to invalid set of [SEP] [CLS] [MASK] df [MASK] data [MASK] packet responderization block b lk of size from [SEP] [CLS] info rm communicator al locator org apache had oop map reduce v [MASK] rm rm container al locator re calculating [MASK] headroom memory v cores [SEP] [CLS] info df s fs name system block name system delete b [MASK] is added [MASK] invalid set [MASK] [SEP] [CLS] info df s [MASK] node packet responder received block b lk of size [MASK] [SEP] [CLS] r m nf c j u r lose [MASK] c j u [MASK] kernel info iar dear c c [SEP] [CLS] info df s fs [MASK] set deleting [MASK] b lk file m nt had oop df s data current sub dir b [MASK] [SEP] [CLS] r [MASK] nf c left u r [MASK] nf c j u ras kernel info iar dear bec [SEP] [CLS] null [MASK] [MASK] m mcs error ido proxy [MASK] hit assert condition assert expression [MASK]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=log_tokenizer_w_n, mlm_probability=0.15)\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {log_tokenizer_w_n.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5e9c21-81cc-4ab1-b951-b0590b946924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_datasets_from_disk = load_from_disk(\"./logdata/logdata_for_pretraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f7aa81-5d5f-4dec-b05f-28c5709403d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "#Unilog元論文(https://arxiv.org/pdf/2112.03159.pdf)の表2の連続マスクを作成するための関数\n",
    "#ランダムでマスクするインデックスの前後もマスクするようにした\n",
    "def make_consecutive_mask(mask):\n",
    "    new_mask = copy.deepcopy(mask)\n",
    "    \n",
    "    masked_indexes = np.where(mask)[0]\n",
    "    \n",
    "    for i, idx in enumerate(masked_indexes):\n",
    "        if idx - 1 >= 0 and idx + 1 < len(new_mask):\n",
    "            if i == 0 or abs(idx - masked_indexes[i-1]) > 3:\n",
    "                new_mask[idx - 1] = 1\n",
    "                new_mask[idx + 1] = 1\n",
    "            else:\n",
    "                new_mask[idx] = 0\n",
    "        else:\n",
    "            new_mask[idx] = 0\n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f6fc75-1dd0-4c0f-8db7-73adc6d2df45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wwm_probability = 0.10\n",
    "\n",
    "#参考https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt\n",
    "def consecutive_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        # CHANGED\n",
    "        # 論文準拠にマスクを連続的にした\n",
    "        mask = make_consecutive_mask(mask)\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = log_tokenizer_w_n.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "    # print(features)\n",
    "    return default_data_collator(features)\n",
    "\n",
    "def consecutive_word_masking_data_collator_with(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        # CHANGED\n",
    "        # 論文準拠にマスクを連続的にした\n",
    "        mask = make_consecutive_mask(mask)\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = log_tokenizer_w_n.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "    print(features)\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bca825-596c-4739-b0c8-eb30e3661975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] info [MASK] [MASK] [MASK] node packet responder received [MASK] [MASK] [MASK] of size from [SEP] [CLS] r m n c j u r m n c j u ras kernel info microseconds spent in the rb s signal handler during calls microseconds was the maximum [MASK] [MASK] [MASK] single instance of a correctable [MASK] [SEP] [CLS] [MASK] [MASK] s fs data set deleting block b lk file m nt had oop df s data current sub dir b [MASK] [SEP] [CLS] [MASK] [MASK] s fs [MASK] [MASK] [MASK] name system add stored block block map [MASK] [MASK] [MASK] to b lk size [SEP] [CLS] info df s data node [MASK] [MASK] ce iver receiving block b lk s rc de st [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop [MASK] [MASK] [MASK] current sub dir b lk [SEP] [CLS] r m ne c j u r m [MASK] [MASK] [MASK] u ras kernel [MASK] [MASK] [MASK] z input pipe error s d cr x f detected and corrected [SEP] [CLS]'\n",
      "\n",
      "'>>> info df s fs name system [MASK] [MASK] [MASK] delete b lk is added to invalid set of [SEP] [CLS] [MASK] [MASK] [MASK] data node packet responder received block b lk of size [MASK] [SEP] [CLS] [MASK] [MASK] communicator al locator [MASK] [MASK] [MASK] oop map reduce [MASK] [MASK] [MASK] rm container al [MASK] [MASK] [MASK] schedule headroom memory v cores [SEP] [CLS] info df s fs name system block name system delete b lk is added to invalid set of [SEP] [CLS] [MASK] [MASK] [MASK] data node packet responder received block b lk of size from [SEP] [CLS] r m nf c j [MASK] [MASK] [MASK] nf c j u ras kernel info iar [MASK] [MASK] [MASK] [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir b lk [SEP] [CLS] r m nf c [MASK] [MASK] [MASK] m nf c j [MASK] [MASK] [MASK] info iar dear bec [SEP] [CLS] null null [MASK] [MASK] [MASK] error ido proxy db hit assert condition assert expression source'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets_from_disk[i] for i in range(2)]\n",
    "batch = consecutive_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {log_tokenizer_w_n.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c53c15a-7b52-48f3-accd-2e1892882f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets = tokenized_datasets.map(add_labels_column, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe484c6-933e-49be-b075-20a092994769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# samples = [tokenized_datasets[i] for i in range(2)]\n",
    "# batch = consecutive_word_masking_data_collator(samples)\n",
    "\n",
    "# for chunk in batch[\"input_ids\"]:\n",
    "#     print(f\"\\n'>>> {log_tokenizer_w_n.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9465acf-7c59-4636-9605-29bdc0701fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForPreTraining\n",
    "from hf_transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996220c4-75c4-4f23-a7c1-7aa5725b0f68",
   "metadata": {},
   "source": [
    "# 訓練用のパラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ecb756-cc05-40a7-bd61-123cb312e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets_from_disk.train_test_split(test_size=0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca7007c-3e2f-4056-af57-90c6d6d09a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1367826\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 151981\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1654ea89-1be6-44df-98fa-8985ebced191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 1367826\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d088624-1923-4baa-b20a-90325bffc6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=consecutive_word_masking_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e75995d-b4f3-471b-bd18-c61a84054809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset_small = lm_datasets[\"train\"].select(range(batch_size*3))\n",
    "# train_dataloader_small = DataLoader(\n",
    "#     train_dataset_small,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size,\n",
    "#     collate_fn=consecutive_word_masking_data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f5394e-906e-45fc-b78b-c067daf61955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = consecutive_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20bec0a1-33da-4906-a7cd-8919148686bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "eval_dataset = lm_datasets[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=lm_datasets[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44df37da-fd47-4d36-8981-912ea63f265b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d1eb94-0916-4ad3-8f25-3956c39c879b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_dataloader_small = DataLoader(\n",
    "#     eval_dataset.select(range(batch_size*3)), batch_size=batch_size, collate_fn=default_data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a6fb33e-9440-48e4-a02e-7301e1a792a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"is_only_mlm\": false,\n",
       "  \"is_unilog\": true,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4075\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unilog元論文の実験に準拠\n",
    "#https://arxiv.org/pdf/2112.03159.pdf\n",
    "unilogConfig= BertConfig(\n",
    "    is_unilog=True,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    num_attention_heads=4,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    vocab_size=log_tokenizer_w_n.vocab_size,\n",
    "    num_hidden_layers=3\n",
    ")\n",
    "unilogConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d61c0f7-c730-4c61-a2af-4ddcabd1f274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"is_only_mlm\": true,\n",
       "  \"is_unilog\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4075\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertConfig= BertConfig(\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    num_attention_heads=4,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    vocab_size=log_tokenizer_w_n.vocab_size,\n",
    "    num_hidden_layers=12,\n",
    "    is_only_mlm=True\n",
    ")\n",
    "bertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54771836-9a6f-456d-98af-d111c46472cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "model = BertForPreTraining(config=bertConfig).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "num_train_steps = 2**16 / (len(lm_datasets[\"train\"]) // batch_size)\n",
    "\n",
    "num_train_epochs = int(np.ceil(num_train_steps))\n",
    "decayRate = 0.96\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6d08e26-a3c3-40ac-953d-463655258948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367826"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3345e9-5577-4f00-bdba-67f3807411c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19d686ac-4173-4bf0-bc54-b8d227fac15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForPreTraining\n",
    "\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "# model = AutoModelForPreTraining.from_pretrained(checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f7ee756-0e87-4b90-a179-585e3f147865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10687\n"
     ]
    }
   ],
   "source": [
    "print(num_train_epochs)\n",
    "output_dir = \"./logdata/bert_log_pretrain_12_layers\"\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50f905-ba56-4efd-8a09-dc516dfca0f9",
   "metadata": {},
   "source": [
    "# 事前学習開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddeae64-b3eb-479c-8b34-fa131a2e49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_train_epochs))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        pbar.set_description(f\">>> Loss {loss}\")\n",
    "        pbar.update(1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    # try:\n",
    "    perplexity = math.exp(torch.mean(losses))\n",
    "    # except OverflowError:\n",
    "    #     perplexity = float(\"inf\")\n",
    "    progress_bar.update(1)\n",
    "    progress_bar.write(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    # Save and upload\n",
    "    model.save_pretrained(output_dir + f\"_{epoch}\")\n",
    "    # if accelerator.is_main_process:\n",
    "        # tokenizer.save_pretrained(output_dir)\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97114968-8a0a-4a29-8286-e461ecc3ca06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch[\"input_ids\"].shape\n",
    "# for chunk in batch[\"input_ids\"]:\n",
    "#     print(f\"\\n'>>> {log_tokenizer_w_n.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d43c5-a952-43b0-808c-42cdcb323c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ff492cf-bc91-4252-8efd-9046016ecd51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10687 [00:00<?, ?it/s]\u001b[A\n",
      ">>> Loss 8.312609672546387:   0%|          | 0/10687 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/10687 [00:00<?, ?it/s]\u001b[A\n",
      ">>> Loss 4.698516845703125:   0%|          | 0/10687 [00:00<?, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "untrained_model = BertForPreTraining(config=unilogConfig).to(device)\n",
    "model6 = BertForPreTraining.from_pretrained(f\"./logdata/unilog_pretrain_preln_on_attentions_6/\").to(device)\n",
    "\n",
    "pbar = tqdm(train_dataloader)\n",
    "for batch in pbar:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = untrained_model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    pbar.set_description(f\">>> Loss {loss}\")\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    break\n",
    "\n",
    "i = 0\n",
    "# for name, p in untrained_model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(p.grad.data)\n",
    "#     i += 1\n",
    "    \n",
    "#     if i == 5:\n",
    "#         break\n",
    "\n",
    "pbar = tqdm(train_dataloader)\n",
    "for batch in pbar:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model6(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    pbar.set_description(f\">>> Loss {loss}\")\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    break\n",
    "i = 0\n",
    "# for name, p in model6.named_parameters():\n",
    "#     print(name)\n",
    "    \n",
    "#     print(p.grad.data)\n",
    "#     i += 1\n",
    "    \n",
    "#     if i == 5:\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d86819d2-ec07-4ae0-9836-a6c9378bbfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model0 = BertForPreTraining.from_pretrained(f\"./logdata/unilog_pretrain_{epoch}/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a53a6c50-bc03-4b34-b807-d95efa915f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param0, param6 in zip(model0.parameters(), model6.parameters()):\n",
    "    print(param0 == param6)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ec8d79f-c31d-41c2-98df-e95efa464eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                        \n",
      "  0%|          | 0/3 [00:52<?, ?it/s]/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 6: Perplexity: 103.80807823718301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# progress_bar = tqdm(range(num_train_epochs))\n",
    "# model6 = BertForPreTraining.from_pretrained(\"./logdata/unilog_pretrain_6/\").to(device)\n",
    "for epoch in tqdm(range(6,3,-1)):\n",
    "    model6 = BertForPreTraining.from_pretrained(f\"./logdata/unilog_pretrain_preln_on_attentions_{epoch}/\").to(device)\n",
    "    gc.collect()\n",
    "    model6.eval()\n",
    "    losses = []\n",
    "\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model6(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        # pbar.update(1)\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    # try:\n",
    "    perplexity = math.exp(torch.mean(losses))\n",
    "    # except OverflowError:\n",
    "    #     perplexity = float(\"inf\")\n",
    "\n",
    "    progress_bar.write(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22b00caf-6c36-4404-b8a7-1829c9065e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_model = BertForPreTraining.from_pretrained(\"./logdata/unilog_pretrain_preln_on_attentions_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13081a-7318-45ee-8646-5b95d650de2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = loss.repeat(batch_size)\n",
    "b = loss.repeat(batch_size)\n",
    "torch.cat([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d0e35-4f42-4f18-8d44-681395d80e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
