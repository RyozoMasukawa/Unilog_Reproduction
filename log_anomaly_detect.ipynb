{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848dabf6-49fa-411b-9477-4bce9ca90d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import wordninja\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, default_data_collator, get_scheduler\n",
    "from hf_transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForPreTraining\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3b51585-c0d0-4c35-acfa-6df8484757b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hdfs_anomaly = pd.read_csv(\"./logdata/hdfs/anomaly_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c7aedb-0533-4876-a02a-a9e10a456fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 553/11175629 [00:37<101:05:47, 30.71it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203518 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203519 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203519 145 INFO dfs.DataNode$PacketResp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175624</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081111 110423 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175625</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081111 110423 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175626</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081111 110423 33 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175627</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081111 110423 33 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175628</th>\n",
       "      <td>blk_-9128742458709757181</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081111 110624 19 INFO dfs.FSDataset: Deleting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11175629 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           BlockId    Label  \\\n",
       "0         blk_-1608999687919862906   Normal   \n",
       "1         blk_-1608999687919862906   Normal   \n",
       "2         blk_-1608999687919862906   Normal   \n",
       "3         blk_-1608999687919862906   Normal   \n",
       "4         blk_-1608999687919862906   Normal   \n",
       "...                            ...      ...   \n",
       "11175624  blk_-9128742458709757181  Anomaly   \n",
       "11175625  blk_-9128742458709757181  Anomaly   \n",
       "11175626  blk_-9128742458709757181  Anomaly   \n",
       "11175627  blk_-9128742458709757181  Anomaly   \n",
       "11175628  blk_-9128742458709757181  Anomaly   \n",
       "\n",
       "                                                        log  \n",
       "0         081109 203518 143 INFO dfs.DataNode$DataXceive...  \n",
       "1         081109 203518 35 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "2         081109 203519 143 INFO dfs.DataNode$DataXceive...  \n",
       "3         081109 203519 145 INFO dfs.DataNode$DataXceive...  \n",
       "4         081109 203519 145 INFO dfs.DataNode$PacketResp...  \n",
       "...                                                     ...  \n",
       "11175624  081111 110423 31 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "11175625  081111 110423 31 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "11175626  081111 110423 33 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "11175627  081111 110423 33 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "11175628  081111 110624 19 INFO dfs.FSDataset: Deleting ...  \n",
       "\n",
       "[11175629 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log2list(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "    return lines\n",
    "\n",
    "hdfs = log2list(\"./logdata/hdfs/HDFS.log\")\n",
    "log_label_pairs = {}\n",
    "\n",
    "df_hdfs_logs = pd.DataFrame({\"log\" : hdfs})\n",
    "df_hdfs_logs[\"BlockId\"] = df_hdfs_logs[\"log\"].map(lambda line : re.search(r\"blk_[-]*[0-9]+\", line).group(0))\n",
    "df_hdfs = df_hdfs_anomaly.merge(df_hdfs_logs, how=\"left\", on=[\"BlockId\"])\n",
    "\n",
    "df_hdfs_without_duplicate = df_hdfs.drop_duplicates([\"BlockId\"])\n",
    "df_hdfs_without_duplicate.to_csv(\"./logdata/hdfs/df_hdfs_without_duplicate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "363d69cc-7132-4407-a06b-58060eb547be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_hdfs_without_duplicate[\"text\"] = df_hdfs_without_duplicate[\"log\"].map(preprocess_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3accd1b-045a-420c-8771-36d6821747e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.0.dev0'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b5954f-230a-4488-b152-aacd7cf29623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#分類と事前学習で異なるトークナイザを利用している？\n",
    "#根拠: 例えばHDFSの異常検知の場合, 異常ラベルはログデータのブロック番号と紐づけられている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28f44bce-4cfc-497d-927d-ce3ed5949837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE\n",
    "#ここで記号を区切ってwordninjaで塊除去を行っている\n",
    "#なお, かなり時間がかかるので実行には注意が必要\n",
    "#もう少し賢く書きたかったが, その書き方を思いつく時間とナイーブに実行する時間はどうせ同じぐらいだろう\n",
    "def preprocess_log(text, remove_digits=True):\n",
    "    text = text.replace('[', \" \")\n",
    "    text = text.replace(\".\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\":\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\";\", \" \")\n",
    "    text = text.replace(\"=\", \" \")\n",
    "    text = text.replace(\"*\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.lower()\n",
    "    text = \" \".join(wordninja.split(text))\n",
    "    remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "    if remove_digits:\n",
    "        return remove_num(text)\n",
    "    return text;\n",
    "\n",
    "def preprocess_log_batch(example, remove_digits=True):\n",
    "    return {\n",
    "        \"text\": [preprocess_log(l, remove_digits=remove_digits) for l in example[\"log\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de78902-9a36-4577-9348-42a7a238d3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ad_hdfs = pd.read_csv(\"./logdata/hdfs/hdfs_log_label_pairs.csv\", usecols=[\"log\", \"BlockId\", \"Label\"])\n",
    "# df_hdfs_without_duplicate[\"text\"] = df_hdfs_without_duplicate[\"log\"].map(preprocess_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856862c1-1d56-4f6d-b870-ee5553ae8cce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-81d414b48ce383ec/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1098.27it/s]\n",
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-81d414b48ce383ec/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['log', 'BlockId', 'Label'],\n",
       "        num_rows: 8940503\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['log', 'BlockId', 'Label'],\n",
       "        num_rows: 2235126\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datased_ad_hdfs = load_dataset(\"csv\", data_files=\"./logdata/hdfs/hdfs_log_label_pairs.csv\")\n",
    "datased_ad_hdfs = datased_ad_hdfs[\"train\"].train_test_split(0.2)\n",
    "datased_ad_hdfs = datased_ad_hdfs.remove_columns(['Unnamed: 0'])\n",
    "datased_ad_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ccd07a4-ccaa-47e6-b344-bc02610afd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1325ab7ded81eb20/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6150.01it/s]\n",
      "\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 857.20it/s]\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating train split: 50000 examples [00:00, 431756.18 examples/s]\u001b[A\n",
      "Generating train split: 100000 examples [00:00, 414683.88 examples/s]\u001b[A\n",
      "Generating train split: 160000 examples [00:00, 452425.90 examples/s]\u001b[A\n",
      "Generating train split: 220000 examples [00:00, 487579.96 examples/s]\u001b[A\n",
      "Generating train split: 280000 examples [00:00, 501330.62 examples/s]\u001b[A\n",
      "Generating train split: 340000 examples [00:00, 499879.29 examples/s]\u001b[A\n",
      "Generating train split: 400000 examples [00:00, 498635.30 examples/s]\u001b[A\n",
      "Generating train split: 460000 examples [00:00, 507186.77 examples/s]\u001b[A\n",
      "Generating train split: 520000 examples [00:01, 509134.14 examples/s]\u001b[A\n",
      "Generating train split: 575061 examples [00:01, 502304.49 examples/s]\u001b[A\n",
      "                                                                     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1325ab7ded81eb20/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.18it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_hdfs = load_dataset(\"csv\", data_files=\"./logdata/hdfs/df_hdfs_without_duplicate.csv\")\n",
    "dataset_hdfs = dataset_hdfs[\"train\"].train_test_split(0.2)\n",
    "dataset_hdfs = dataset_hdfs.remove_columns(['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cccef4d6-b023-474c-bea1-818afdbaa14b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['BlockId', 'Label', 'log'],\n",
       "        num_rows: 460048\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['BlockId', 'Label', 'log'],\n",
       "        num_rows: 115013\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf549ef3-4376-4c63-b884-5e0220d567d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['BlockId', 'Label', 'log'],\n",
       "        num_rows: 57506\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['BlockId', 'Label', 'log'],\n",
       "        num_rows: 57507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset_hdfs[\"train\"]\n",
    "test_valid = dataset_hdfs[\"test\"].train_test_split(0.5)\n",
    "test_dataset = test_valid[\"test\"]\n",
    "eval_dataset = test_valid[\"train\"]\n",
    "test_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94ff93d3-8583-4520-8ab1-2deb6e76be68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['BlockId', 'Label', 'log', 'text'],\n",
       "    num_rows: 460048\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732b8c1-9752-440e-a0f5-3add2c0389a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f09fe6e7-8c2a-414a-a639-32f52ae288ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:  96%|█████████▌| 55000/57507 [00:51<00:02, 1073.35 examples/s]\u001b[A\n",
      "Map:  97%|█████████▋| 56000/57507 [00:52<00:01, 1074.45 examples/s]\u001b[A\n",
      "Map:  99%|█████████▉| 57000/57507 [00:53<00:00, 1078.44 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 57507/57507 [00:53<00:00, 1070.86 examples/s]\u001b[A\n",
      "                                                                   \u001b[A"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60692afd-fd11-4a55-a918-aea412a2a04f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['log', 'BlockId', 'Label'],\n",
       "    num_rows: 8940503\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "964067ad-2654-4dab-a8fb-92ce482fe04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_large/\")\n",
    "log_tokenizer_w_n = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_without_numbers/\")\n",
    "\n",
    "batch_size=16  # change to 16 for full training\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    batch[\"labels\"] = list(map(lambda x : int(x == \"Normal\"), batch[\"Label\"]))\n",
    "\n",
    "    return log_tokenizer_w_n(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=180)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d396f-5e22-472c-babc-352ac0945a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "tokenized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52e6be99-b353-49d8-be8a-1d7fd126bae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_data.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9767ba64-f052-4ddf-85a1-07c2e4c0cf96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_data.shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328aff9-e1fe-44b5-8a19-dbf7ede9b900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset.set_format(type=\"torch\")\n",
    "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "179f9999-de31-49d3-9c59-ad65d1504fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset.set_format(type=\"torch\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3afb3d03-0809-4a5a-a602-9e34ab4a3d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_train_data, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6272cde-7c5d-4e93-88cb-75a14511b6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3dd962f7-93dd-458d-913e-eb54ca64c6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./logdata/unilog_pretrain_preln_on_attentions_0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unilogConfigmodel = BertForSequenceClassification.from_pretrained(\"./logdata/unilog_pretrain_preln_on_attentions_0/\", num_labels=2).to(device)\n",
    "optimizer = AdamW(unilogConfigmodel.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e5e97cdc-d0af-4035-be87-59b2d3fdd860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e2c2fa4a-5b24-4887-a2d3-19795e2e7ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bb13d5c0-01ae-4a3f-99b9-91ad8eb831f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7295932d-dac3-4ff2-9bf2-484e78fae454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af34ae4-b0d8-49f6-a68a-6a03a3231ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57506/57506 [19:17<00:00, 49.66it/s]\n",
      "100%|██████████| 7189/7189 [00:47<00:00, 151.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9707508781692346}\n",
      "{'precision': 0.9707508781692346}\n",
      "{'recall': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3421/57506 [01:05<15:46, 57.15it/s]"
     ]
    }
   ],
   "source": [
    "train_steps = 0\n",
    "\n",
    "save_steps = 10000\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    unilogConfigmodel.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        train_steps += 1\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = unilogConfigmodel(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if train_steps % save_steps == 0:\n",
    "            unilogConfigmodel.save_pretrained(f\"./logdata/log_anomaly_model_step_{train_steps}\")\n",
    "    import evaluate\n",
    "\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_precision = evaluate.load(\"precision\")\n",
    "    metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "    unilogConfigmodel.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = unilogConfigmodel(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    print(metric_acc.compute())\n",
    "    print(metric_precision.compute())\n",
    "    print(metric_recall.compute())\n",
    "    \n",
    "    curr_f1_score = metric_f1.compute()[\"f1\"]\n",
    "    prev_f1_score = best_f1\n",
    "    if (curr_f1_score > prev_f1_score):\n",
    "        best_f1 = max(best_f1, curr_f1_score)\n",
    "        unilogConfigmodel.save_pretrained(f\"./logdata/log_anomaly_model_best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fae2252b-f4c3-4980-a14b-f6e7a6c6991a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9705427165388562}\n",
      "{'precision': 0.9705427165388562}\n",
      "{'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "unilogConfigmodel.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = unilogConfigmodel(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    \n",
    "print(metric_acc.compute())\n",
    "# print(metric_f1.compute())\n",
    "print(metric_precision.compute())\n",
    "print(metric_recall.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "df46d497-7bf2-47e9-904d-3e720ca5d5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850511824920578"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cb3fba22-a7ac-4ea0-8206-efef0f6d2613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"f1\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n",
       "    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n",
       "    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n",
       "\n",
       "        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n",
       "        - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
       "        - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
       "        - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n",
       "        - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    f1 (`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple binary example\n",
       "        >>> f1_metric = evaluate.load(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'f1': 0.5}\n",
       "\n",
       "    Example 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\n",
       "        >>> f1_metric = evaluate.load(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.67\n",
       "\n",
       "    Example 3-The same simple binary example as in Example 1, but with `sample_weight` included.\n",
       "        >>> f1_metric = evaluate.load(\"f1\")\n",
       "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.35\n",
       "\n",
       "    Example 4-A multiclass example, with different values for the `average` input.\n",
       "        >>> predictions = [0, 2, 1, 0, 0, 1]\n",
       "        >>> references = [0, 1, 2, 0, 1, 2]\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.27\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.33\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.27\n",
       "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\n",
       "        >>> print(results)\n",
       "        {'f1': array([0.8, 0. , 0. ])}\n",
       "\n",
       "    Example 5-A multi-label example\n",
       "        >>> f1_metric = evaluate.load(\"f1\", \"multilabel\")\n",
       "        >>> results = f1_metric.compute(predictions=[[0, 1, 1], [1, 1, 0]], references=[[0, 1, 1], [0, 1, 0]], average=\"macro\")\n",
       "        >>> print(round(results['f1'], 2))\n",
       "        0.67\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
