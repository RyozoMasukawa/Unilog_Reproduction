{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848dabf6-49fa-411b-9477-4bce9ca90d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import wordninja\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, default_data_collator, get_scheduler\n",
    "from hf_transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForPreTraining\n",
    "from torch.utils.data import DataLoader\n",
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcd061c-2322-4fdd-83f8-8b331712829e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#WARNING\n",
    "#cacheディレクトリの指定は絶対マウント先のフォルダにするように\n",
    "#そうしないとdockerイメージを管理している研究室サーバーのルートディレクトリ(/)がパンパンになってしまう\n",
    "#/が容量オーバーすると何も動かなくなって他の人に迷惑\n",
    "!export HF_DATASETS_CACHE=\"/home/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7afe5ec-0bfb-4a81-ad68-02e9e534b50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hdfs_without_duplicate = pd.read_csv(\"./logdata/hdfs/df_hdfs_without_duplicate.csv\", usecols=[\"BlockId\", \"Label\", \"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d3cc70-ee9a-4e52-bdc6-07911f8f2657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         081109 203518 143 INFO dfs.DataNode$DataXceive...\n",
       "1         081109 203520 142 INFO dfs.DataNode$DataXceive...\n",
       "2         081109 203521 145 INFO dfs.DataNode$DataXceive...\n",
       "3         081109 203523 143 INFO dfs.DataNode$DataXceive...\n",
       "4         081109 203529 148 INFO dfs.DataNode$DataXceive...\n",
       "                                ...                        \n",
       "575056    081111 110351 27174 INFO dfs.DataNode$DataXcei...\n",
       "575057    081111 110359 26685 INFO dfs.DataNode$DataXcei...\n",
       "575058    081111 110402 27311 INFO dfs.DataNode$DataXcei...\n",
       "575059    081111 110412 27231 INFO dfs.DataNode$DataXcei...\n",
       "575060    081111 110413 26687 INFO dfs.DataNode$DataXcei...\n",
       "Name: log, Length: 575061, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hdfs_without_duplicate[\"log\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "782c9296-2957-4aef-9fba-f1c552a573af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PCAテストデータの読み込み\n",
    "df_hdfs_anomaly_test = pd.read_csv(\"./logdata/loglizer_hdfs_test.csv\", usecols=[\"BlockId\"])\n",
    "pca_test_block_ids = list(df_hdfs_anomaly_test[\"BlockId\"])\n",
    "df_cmp_loglizer_test = df_hdfs_without_duplicate[df_hdfs_without_duplicate[\"BlockId\"].isin(pca_test_block_ids)]\n",
    "df_cmp_loglizer_test.to_csv(\"./logdata/hdfs/loglizer_hdfs_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0076d9ed-f509-446c-a33f-eb18bbc93db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PCA訓練データの読み込み\n",
    "df_hdfs_anomaly_train = pd.read_csv(\"./logdata/loglizer_hdfs_train.csv\", usecols=[\"BlockId\"])\n",
    "pca_train_block_ids = list(df_hdfs_anomaly_train[\"BlockId\"])\n",
    "df_cmp_loglizer_train = df_hdfs_without_duplicate[df_hdfs_without_duplicate[\"BlockId\"].isin(pca_train_block_ids)]\n",
    "df_cmp_loglizer_train.to_csv(\"./logdata/hdfs/loglizer_hdfs_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e58a350-ff77-4e97-8838-39a6cd959138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203518 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203520 142 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 203521 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203523 143 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 203529 148 INFO dfs.DataNode$DataXceive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>blk_8608974154783629388</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204328 30 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>blk_8225451996519902271</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204330 30 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>blk_-5350492094942566244</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204330 34 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>blk_4438299806816545743</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204335 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>blk_3061937980832296286</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204344 32 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3969 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       BlockId    Label  \\\n",
       "0     blk_-1608999687919862906   Normal   \n",
       "1      blk_7503483334202473044   Normal   \n",
       "2     blk_-3544583377289625738  Anomaly   \n",
       "3     blk_-9073992586687739851   Normal   \n",
       "4      blk_7854771516489510256   Normal   \n",
       "...                        ...      ...   \n",
       "4714   blk_8608974154783629388  Anomaly   \n",
       "4728   blk_8225451996519902271  Anomaly   \n",
       "4732  blk_-5350492094942566244  Anomaly   \n",
       "4780   blk_4438299806816545743  Anomaly   \n",
       "4869   blk_3061937980832296286  Anomaly   \n",
       "\n",
       "                                                    log  \n",
       "0     081109 203518 143 INFO dfs.DataNode$DataXceive...  \n",
       "1     081109 203520 142 INFO dfs.DataNode$DataXceive...  \n",
       "2     081109 203521 145 INFO dfs.DataNode$DataXceive...  \n",
       "3     081109 203523 143 INFO dfs.DataNode$DataXceive...  \n",
       "4     081109 203529 148 INFO dfs.DataNode$DataXceive...  \n",
       "...                                                 ...  \n",
       "4714  081109 204328 30 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "4728  081109 204330 30 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "4732  081109 204330 34 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "4780  081109 204335 31 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "4869  081109 204344 32 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "\n",
       "[3969 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cmp_loglizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f984c0f2-6fb8-4f22-9261-3405cf7e4ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO dfs.DataNode$DataXceiver : Receiving bloc...</td>\n",
       "      <td>Receiving block src; blockMap updated; Verific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFO dfs.DataNode$DataXceiver : Receiving bloc...</td>\n",
       "      <td>Receiving block src; blockMap updated; Verific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFO dfs.DataNode$DataXceiver : Receiving bloc...</td>\n",
       "      <td>Receiving block src; blockMap updated; Verific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFO dfs.DataNode$DataXceiver : Receiving bloc...</td>\n",
       "      <td>Receiving block src; blockMap updated; Verific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO dfs.DataNode$PacketResponder : PacketResp...</td>\n",
       "      <td>Receiving block src; blockMap updated; Verific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>INFO storage.BlockManager : Removing RDD 12744</td>\n",
       "      <td>Removing RDD;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>INFO storage.BlockManager : Removing RDD 12740</td>\n",
       "      <td>Removing RDD;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>INFO storage.BlockManager : Removing RDD 12736</td>\n",
       "      <td>Removing RDD;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>INFO storage.BlockManager : Removing RDD 12732</td>\n",
       "      <td>Removing RDD;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>INFO storage.BlockManager : Removing RDD 12728</td>\n",
       "      <td>Removing RDD;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0      INFO dfs.DataNode$DataXceiver : Receiving bloc...   \n",
       "1      INFO dfs.DataNode$DataXceiver : Receiving bloc...   \n",
       "2      INFO dfs.DataNode$DataXceiver : Receiving bloc...   \n",
       "3      INFO dfs.DataNode$DataXceiver : Receiving bloc...   \n",
       "4      INFO dfs.DataNode$PacketResponder : PacketResp...   \n",
       "...                                                  ...   \n",
       "11975     INFO storage.BlockManager : Removing RDD 12744   \n",
       "11976     INFO storage.BlockManager : Removing RDD 12740   \n",
       "11977     INFO storage.BlockManager : Removing RDD 12736   \n",
       "11978     INFO storage.BlockManager : Removing RDD 12732   \n",
       "11979     INFO storage.BlockManager : Removing RDD 12728   \n",
       "\n",
       "                                                 summary  \n",
       "0      Receiving block src; blockMap updated; Verific...  \n",
       "1      Receiving block src; blockMap updated; Verific...  \n",
       "2      Receiving block src; blockMap updated; Verific...  \n",
       "3      Receiving block src; blockMap updated; Verific...  \n",
       "4      Receiving block src; blockMap updated; Verific...  \n",
       "...                                                  ...  \n",
       "11975                                      Removing RDD;  \n",
       "11976                                      Removing RDD;  \n",
       "11977                                      Removing RDD;  \n",
       "11978                                      Removing RDD;  \n",
       "11979                                      Removing RDD;  \n",
       "\n",
       "[11980 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_summary[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3451d095-823f-4f28-84d1-56eb81710f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>blk_942782950815104487</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204208 26 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>blk_1819115441742884087</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204208 26 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>blk_7082151245718002140</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204208 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>blk_-3727175694322392368</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204208 32 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>blk_-7300835887062415762</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204208 33 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>blk_-1445970677921829671</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204836 29 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>blk_-5943236831140622436</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204836 30 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>blk_-5039164935117450945</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204836 30 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7938</th>\n",
       "      <td>blk_7379833155074044619</td>\n",
       "      <td>Normal</td>\n",
       "      <td>081109 204836 33 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7939</th>\n",
       "      <td>blk_8909107483987085802</td>\n",
       "      <td>Anomaly</td>\n",
       "      <td>081109 204836 34 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3971 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       BlockId    Label  \\\n",
       "3929    blk_942782950815104487   Normal   \n",
       "3930   blk_1819115441742884087   Normal   \n",
       "3931   blk_7082151245718002140   Normal   \n",
       "3932  blk_-3727175694322392368   Normal   \n",
       "3933  blk_-7300835887062415762   Normal   \n",
       "...                        ...      ...   \n",
       "7935  blk_-1445970677921829671   Normal   \n",
       "7936  blk_-5943236831140622436   Normal   \n",
       "7937  blk_-5039164935117450945   Normal   \n",
       "7938   blk_7379833155074044619   Normal   \n",
       "7939   blk_8909107483987085802  Anomaly   \n",
       "\n",
       "                                                    log  \n",
       "3929  081109 204208 26 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "3930  081109 204208 26 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "3931  081109 204208 31 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "3932  081109 204208 32 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "3933  081109 204208 33 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "...                                                 ...  \n",
       "7935  081109 204836 29 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "7936  081109 204836 30 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "7937  081109 204836 30 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "7938  081109 204836 33 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "7939  081109 204836 34 INFO dfs.FSNamesystem: BLOCK*...  \n",
       "\n",
       "[3971 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cmp_loglizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c7aedb-0533-4876-a02a-a9e10a456fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def log2list(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         lines = [line.rstrip(\"\\n\") for line in f]\n",
    "#     return lines\n",
    "\n",
    "# hdfs = log2list(\"./logdata/hdfs/HDFS.log\")\n",
    "# log_label_pairs = {}\n",
    "\n",
    "# df_hdfs_logs = pd.DataFrame({\"log\" : hdfs})\n",
    "# df_hdfs_logs[\"BlockId\"] = df_hdfs_logs[\"log\"].map(lambda line : re.search(r\"blk_[-]*[0-9]+\", line).group(0))\n",
    "# df_hdfs = df_hdfs_anomaly.merge(df_hdfs_logs, how=\"left\", on=[\"BlockId\"])\n",
    "\n",
    "# df_hdfs_without_duplicate = df_hdfs.drop_duplicates([\"BlockId\"])\n",
    "# df_hdfs_without_duplicate.to_csv(\"./logdata/hdfs/df_hdfs_without_duplicate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363d69cc-7132-4407-a06b-58060eb547be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_hdfs_without_duplicate[\"text\"] = df_hdfs_without_duplicate[\"log\"].map(preprocess_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3accd1b-045a-420c-8771-36d6821747e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.0.dev0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b5954f-230a-4488-b152-aacd7cf29623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#分類と事前学習で異なるトークナイザを利用している？\n",
    "#根拠: 例えばHDFSの異常検知の場合, 異常ラベルはログデータのブロック番号と紐づけられている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28f44bce-4cfc-497d-927d-ce3ed5949837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE\n",
    "#ここで記号を区切ってwordninjaで塊除去を行っている\n",
    "#なお, かなり時間がかかるので実行には注意が必要\n",
    "#もう少し賢く書きたかったが, その書き方を思いつく時間とナイーブに実行する時間はどうせ同じぐらいだろう\n",
    "def preprocess_log(text, remove_digits=True):\n",
    "    text = text.replace('[', \" \")\n",
    "    text = text.replace(\".\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\":\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\";\", \" \")\n",
    "    text = text.replace(\"=\", \" \")\n",
    "    text = text.replace(\"*\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.lower()\n",
    "    text = \" \".join(wordninja.split(text))\n",
    "    remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "    if remove_digits:\n",
    "        return remove_num(text)\n",
    "    return text;\n",
    "\n",
    "def preprocess_log_batch(example, remove_digits=True):\n",
    "    return {\n",
    "        \"text\": [preprocess_log(l, remove_digits=remove_digits) for l in example[\"log\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4de78902-9a36-4577-9348-42a7a238d3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ad_hdfs = pd.read_csv(\"./logdata/hdfs/hdfs_log_label_pairs.csv\", usecols=[\"log\", \"BlockId\", \"Label\"])\n",
    "# df_hdfs_without_duplicate[\"text\"] = df_hdfs_without_duplicate[\"log\"].map(preprocess_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b85234e-f3b4-4217-a0cd-30408e714cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal     0.974207\n",
       "Anomaly    0.025793\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ad_hdfs[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f4e7758-766b-40d2-9682-f9d783b77fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>081109 203521 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>081109 203521 29 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>081109 203522 144 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>081109 203522 145 INFO dfs.DataNode$DataXceive...</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>081109 203522 147 INFO dfs.DataNode$PacketResp...</td>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605276</th>\n",
       "      <td>081111 030649 32 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_4241722502979736601</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485026</th>\n",
       "      <td>081110 010143 30 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_2863437573282244260</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7182233</th>\n",
       "      <td>081111 040435 33 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_-8441598874045640693</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684416</th>\n",
       "      <td>081111 044332 29 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_795775988001366663</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539111</th>\n",
       "      <td>081110 210122 28 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
       "      <td>blk_3806682485949045504</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       log  \\\n",
       "291      081109 203521 145 INFO dfs.DataNode$DataXceive...   \n",
       "292      081109 203521 29 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "293      081109 203522 144 INFO dfs.DataNode$DataXceive...   \n",
       "294      081109 203522 145 INFO dfs.DataNode$DataXceive...   \n",
       "295      081109 203522 147 INFO dfs.DataNode$PacketResp...   \n",
       "...                                                    ...   \n",
       "6605276  081111 030649 32 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "1485026  081110 010143 30 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "7182233  081111 040435 33 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "5684416  081111 044332 29 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "4539111  081110 210122 28 INFO dfs.FSNamesystem: BLOCK*...   \n",
       "\n",
       "                          BlockId    Label  \n",
       "291      blk_-3544583377289625738  Anomaly  \n",
       "292      blk_-3544583377289625738  Anomaly  \n",
       "293      blk_-3544583377289625738  Anomaly  \n",
       "294      blk_-3544583377289625738  Anomaly  \n",
       "295      blk_-3544583377289625738  Anomaly  \n",
       "...                           ...      ...  \n",
       "6605276   blk_4241722502979736601   Normal  \n",
       "1485026   blk_2863437573282244260   Normal  \n",
       "7182233  blk_-8441598874045640693   Normal  \n",
       "5684416    blk_795775988001366663   Normal  \n",
       "4539111   blk_3806682485949045504   Normal  \n",
       "\n",
       "[576500 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled = df_ad_hdfs[df_ad_hdfs[\"Label\"]==\"Anomaly\"]\n",
    "df_normal_sampled = df_ad_hdfs[df_ad_hdfs[\"Label\"]==\"Normal\"].sample(len(df_downsampled))\n",
    "df_downsampled = pd.concat([df_downsampled, df_normal_sampled])\n",
    "df_downsampled.to_csv(\"./logdata/downsampled_anomaly.csv\")\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ccd07a4-ccaa-47e6-b344-bc02610afd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-a438c5b2cd2acdf4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 701.51it/s]\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-da9900ce5058004e/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 487.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#NOTE\n",
    "#Loading train test dataset used in loglizer here for the sake of fair comparison\n",
    "\n",
    "TEST_DATA_FILE_PATH = \"./logdata/hdfs/loglizer_hdfs_test_data.csv\"\n",
    "TRAIN_DATA_FILE_PATH = \"./logdata/hdfs/loglizer_hdfs_train_data.csv\"\n",
    "\n",
    "\n",
    "dataset_hdfs_test = load_dataset(\"csv\", data_files=TEST_DATA_FILE_PATH)\n",
    "dataset_hdfs_test = dataset_hdfs_test.remove_columns(['Unnamed: 0'])\n",
    "\n",
    "dataset_hdfs_train = load_dataset(\"csv\", data_files=TRAIN_DATA_FILE_PATH)\n",
    "dataset_hdfs_train = dataset_hdfs_train.remove_columns(['Unnamed: 0'])\n",
    "\n",
    "train_valid = dataset_hdfs_train[\"train\"].train_test_split(0.2)\n",
    "train_data = train_valid[\"train\"]\n",
    "eval_dataset = train_valid[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f04908e-47e3-4643-86a7-f5a1c86272fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NOTE\n",
    "#Removing all test data here\n",
    "\n",
    "df_hdfs_without_duplicate = pd.read_csv(\"./logdata/hdfs/df_hdfs_without_duplicate.csv\", usecols=[\"log\", \"BlockId\", \"Label\"])\n",
    "df_test = pd.read_csv(TEST_DATA_FILE_PATH, usecols=[\"log\", \"BlockId\", \"Label\"])\n",
    "test_blk_ids = list(df_test[\"BlockId\"])\n",
    "df_clean = df_hdfs_without_duplicate[~df_hdfs_without_duplicate[\"BlockId\"].isin(test_blk_ids)]\n",
    "df_clean.to_csv(\"./logdata/hdfs/anomaly_without_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf549ef3-4376-4c63-b884-5e0220d567d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading larger dataset that does not contain test data used in loglizer \n",
    "# datased_ad_hdfs = load_dataset(\"csv\", data_files=\"./logdata/hdfs/anomaly_without_test.csv\")\n",
    "# datased_ad_hdfs = datased_ad_hdfs[\"train\"].train_test_split(0.2)\n",
    "# datased_ad_hdfs = datased_ad_hdfs.remove_columns(['Unnamed: 0'])\n",
    "\n",
    "# train_data = datased_ad_hdfs[\"train\"]\n",
    "# eval_dataset = datased_ad_hdfs[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9732b8c1-9752-440e-a0f5-3add2c0389a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f09fe6e7-8c2a-414a-a639-32f52ae288ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "964067ad-2654-4dab-a8fb-92ce482fe04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_large/\")\n",
    "log_tokenizer_w_n = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_without_numbers/\")\n",
    "\n",
    "batch_size=16  # change to 16 for full training\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    batch[\"labels\"] = list(map(lambda x : int(x != \"Normal\"), batch[\"Label\"]))\n",
    "\n",
    "    return log_tokenizer_w_n(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=180)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e5d396f-5e22-472c-babc-352ac0945a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3175\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=1000, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "tokenized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b5c91e0-4bdd-4d69-8b10-e71509188d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3175\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52e6be99-b353-49d8-be8a-1d7fd126bae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_data.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9767ba64-f052-4ddf-85a1-07c2e4c0cf96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_train_data.shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9328aff9-e1fe-44b5-8a19-dbf7ede9b900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "\n",
    "eval_dataset.set_format(type=\"torch\")\n",
    "# small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "179f9999-de31-49d3-9c59-ad65d1504fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-a438c5b2cd2acdf4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 604.63it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-a438c5b2cd2acdf4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-af2542df15b9a134.arrow\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "dataset_hdfs_test = load_dataset(\"csv\", data_files=TEST_DATA_FILE_PATH)\n",
    "dataset_hdfs_test = dataset_hdfs_test.remove_columns(['Unnamed: 0'])\n",
    "test_dataset = dataset_hdfs_test.map(\n",
    "    preprocess_log_batch,\n",
    "    batched=True,\n",
    ")[\"train\"]\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=['log', 'BlockId', 'Label', 'text']\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset.set_format(type=\"torch\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3afb3d03-0809-4a5a-a602-9e34ab4a3d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(tokenized_train_data, shuffle=True, batch_size=32)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ac61b05-90b9-4047-b103-b7621c3924f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3dd962f7-93dd-458d-913e-eb54ca64c6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./logdata/unilog_pretrain_preln_on_attentions_0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "unilogConfigmodel = BertForSequenceClassification.from_pretrained(\"./logdata/unilog_pretrain_preln_on_attentions_0/\", num_labels=2).to(device)\n",
    "optimizer = AdamW(unilogConfigmodel.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5e97cdc-d0af-4035-be87-59b2d3fdd860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794aa24-4d31-4309-b373-a0089c9f70e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e5a3734-3d7f-4a53-91a1-992ebe3e11d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2c2fa4a-5b24-4887-a2d3-19795e2e7ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9af34ae4-b0d8-49f6-a68a-6a03a3231ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_steps = 0\n",
    "\n",
    "save_steps = 100\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    unilogConfigmodel.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "        train_steps += 1\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = unilogConfigmodel(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        pbar.write(f\"Loss:{loss}\")\n",
    "        pbar.update(1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if train_steps % save_steps == 0:\n",
    "            unilogConfigmodel.save_pretrained(f\"./logdata/log_anomaly_pca_cmp_{train_steps}\")\n",
    "    import evaluate\n",
    "\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_precision = evaluate.load(\"precision\")\n",
    "    metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "    unilogConfigmodel.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = unilogConfigmodel(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    pbar.write(str(metric_acc.compute()))\n",
    "    pbar.write(str(metric_precision.compute()))\n",
    "    pbar.write(str(metric_recall.compute()))\n",
    "    \n",
    "    curr_f1_score = metric_f1.compute()[\"f1\"]\n",
    "    prev_f1_score = best_f1\n",
    "    if (curr_f1_score > prev_f1_score):\n",
    "        best_f1 = max(best_f1, curr_f1_score)\n",
    "        unilogConfigmodel.save_pretrained(f\"./logdata/log_anomaly_model_small_best_0708\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b726437-951e-49da-ac48-4f12366780af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3175\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3902c14-1d0d-44e8-9769-6f5ba6e68446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3971\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fae2252b-f4c3-4980-a14b-f6e7a6c6991a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/497 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 15/497 [00:00<00:03, 143.33it/s]\u001b[A\n",
      "  6%|▌         | 30/497 [00:00<00:03, 142.72it/s]\u001b[A\n",
      "  9%|▉         | 45/497 [00:00<00:03, 142.69it/s]\u001b[A\n",
      " 12%|█▏        | 60/497 [00:00<00:03, 142.67it/s]\u001b[A\n",
      " 15%|█▌        | 75/497 [00:00<00:02, 142.53it/s]\u001b[A\n",
      " 18%|█▊        | 90/497 [00:00<00:02, 142.44it/s]\u001b[A\n",
      " 21%|██        | 105/497 [00:00<00:02, 142.54it/s]\u001b[A\n",
      " 24%|██▍       | 120/497 [00:00<00:02, 142.74it/s]\u001b[A\n",
      " 27%|██▋       | 135/497 [00:00<00:02, 142.64it/s]\u001b[A\n",
      " 30%|███       | 150/497 [00:01<00:02, 142.65it/s]\u001b[A\n",
      " 33%|███▎      | 165/497 [00:01<00:02, 142.39it/s]\u001b[A\n",
      " 36%|███▌      | 180/497 [00:01<00:02, 142.16it/s]\u001b[A\n",
      " 39%|███▉      | 195/497 [00:01<00:02, 143.64it/s]\u001b[A\n",
      " 42%|████▏     | 210/497 [00:01<00:02, 143.11it/s]\u001b[A\n",
      " 45%|████▌     | 225/497 [00:01<00:01, 142.78it/s]\u001b[A\n",
      " 48%|████▊     | 240/497 [00:01<00:01, 142.67it/s]\u001b[A\n",
      " 51%|█████▏    | 255/497 [00:01<00:01, 142.52it/s]\u001b[A\n",
      " 54%|█████▍    | 270/497 [00:01<00:01, 142.16it/s]\u001b[A\n",
      " 57%|█████▋    | 285/497 [00:02<00:01, 141.83it/s]\u001b[A\n",
      " 60%|██████    | 300/497 [00:02<00:01, 141.47it/s]\u001b[A\n",
      " 63%|██████▎   | 315/497 [00:02<00:01, 141.29it/s]\u001b[A\n",
      " 66%|██████▋   | 330/497 [00:02<00:01, 141.23it/s]\u001b[A\n",
      " 69%|██████▉   | 345/497 [00:02<00:01, 141.72it/s]\u001b[A\n",
      " 72%|███████▏  | 360/497 [00:02<00:00, 141.09it/s]\u001b[A\n",
      " 75%|███████▌  | 375/497 [00:02<00:00, 142.87it/s]\u001b[A\n",
      " 78%|███████▊  | 390/497 [00:02<00:00, 142.21it/s]\u001b[A\n",
      " 81%|████████▏ | 405/497 [00:02<00:00, 141.18it/s]\u001b[A\n",
      " 85%|████████▍ | 420/497 [00:02<00:00, 142.06it/s]\u001b[A\n",
      " 88%|████████▊ | 435/497 [00:03<00:00, 142.24it/s]\u001b[A\n",
      " 91%|█████████ | 450/497 [00:03<00:00, 142.62it/s]\u001b[A\n",
      " 94%|█████████▍| 466/497 [00:03<00:00, 145.31it/s]\u001b[A\n",
      " 97%|█████████▋| 481/497 [00:03<00:00, 145.43it/s]\u001b[A\n",
      "100%|██████████| 497/497 [00:03<00:00, 142.78it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9604633593553261}\n",
      "{'f1': 0.0}\n",
      "{'precision': 0.0}\n",
      "{'recall': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "unilogBestModel = BertForSequenceClassification.from_pretrained(\"./logdata/log_anomaly_pca_cmp_300/\", num_labels=2).to(device)\n",
    "unilogBestModel.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = unilogBestModel(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    metric_recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    \n",
    "print(metric_acc.compute())\n",
    "print(metric_f1.compute())\n",
    "print(metric_precision.compute())\n",
    "print(metric_recall.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867664e-7396-4392-8bcf-4a93bd45b724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e2decb0-f9d0-4867-a547-7ece1216b158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.960463\n",
       "1    0.039537\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.to_pandas()[\"labels\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8add446-f237-49f6-a436-5dc17bba4444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.960463\n",
       "0    0.039537\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.dataset.to_pandas()[\"labels\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba5245f-b95f-47e2-99d9-d624b689e740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9604633593553261}\n",
      "{'f1': 0.9798330122029544}\n",
      "{'precision': 0.9604633593553261}\n",
      "{'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"{'accuracy': 0.9604633593553261}\n",
    "{'f1': 0.9798330122029544}\n",
    "{'precision': 0.9604633593553261}\n",
    "{'recall': 1.0}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
