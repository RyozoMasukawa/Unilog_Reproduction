{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "12b81200-d7d9-409d-8c48-8d1d08832b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import wordninja\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, default_data_collator, get_scheduler\n",
    "from hf_transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import preprocess_log_batch_hof\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from rouge import Rouge \n",
    "\n",
    "# Create your views here.\n",
    "openai.api_key='YOUR OPENAI API KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93afff18-0c0d-47de-b0f7-614ee7d1b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING\n",
    "#cacheディレクトリの指定は絶対マウント先のフォルダにするように\n",
    "#そうしないとdockerイメージを管理している研究室サーバーの/がパンパンになってしまう\n",
    "#/が容量オーバーすると何も動かなくなって他の人に迷惑\n",
    "!export HF_DATASETS_CACHE=\"/home/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d21593a-9186-4e90-9aae-571a7df84e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# anomaly_indexes = df_hdfs[df_hdfs[\"Label\"]==\"Anomaly\"].index\n",
    "# window_size = 3\n",
    "# anomaly_logs = {\n",
    "#     \"logs\":[],\n",
    "#     \"BlockId\":[]\n",
    "# }\n",
    "# i=0\n",
    "# for idx in anomaly_indexes:\n",
    "#     blockId = df_hdfs.iloc[idx, :][\"BlockId\"]\n",
    "#     i+=1\n",
    "#     anomaly_logs[\"logs\"].append(\" \".join(list(df_hdfs.iloc[idx-window_size:idx+window_size, :][\"log\"])))\n",
    "#     anomaly_logs[\"BlockId\"].append(blockId)\n",
    "    \n",
    "# df_anomaly_logs = pd.DataFrame(anomaly_logs)\n",
    "# df_anomaly_logs = df_anomaly_logs[df_anomaly_logs[\"logs\"] != \"\"]\n",
    "\n",
    "# def get_blockId(line):\n",
    "#     match = re.search(r\"blk_[-]*[0-9]+\", line)\n",
    "#     if match is not None:\n",
    "#         return match.group(0)\n",
    "#     return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "56ec7100-46d0-443c-8552-567feee215f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#unilog再現実装で事前学習済みのBertモデルを持ってきた\n",
    "unilog_bert_model = BertModel.from_pretrained(\"./logdata/unilog_pretrain_mask15_without_numbers_2/\")\n",
    "\n",
    "#unilog再現実装で訓練したトークナイザ\n",
    "log_tokenizer_w_n = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_without_numbers/\")\n",
    "\n",
    "#unilog形式でファインチューニング済みのBERTでログをベクトルへエンコードする関数\n",
    "def vectorize_using_unilog(text, unilog_model, tokenizer):\n",
    "    sample = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = unilog_model(**sample)\n",
    "    return torch.mean(output[\"last_hidden_state\"], dim=1).squeeze().numpy()\n",
    "\n",
    "#fasttextでログをベクトル化\n",
    "def vectorize_log(target_model):\n",
    "    def vectorize_log_helper(input_log):\n",
    "        v = np.zeros(100, dtype=float)\n",
    "\n",
    "        for w in input_log.split():\n",
    "            v += target_model[w] \n",
    "        return v / len(input_log.split())\n",
    "    return vectorize_log_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e80e-78e0-49e1-a9d5-091ef5aaa0b5",
   "metadata": {},
   "source": [
    "# データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "08c050d6-f9c2-44b1-94fe-9cf266731939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-00dcddb0e73b1160/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 513.38it/s]\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset_summary = load_dataset(\"csv\", data_files=\"./logdata/log_summary_pairs.csv\")\n",
    "dataset_summary = dataset_summary.remove_columns(['Unnamed: 0'])\n",
    "dataset_summary = dataset_summary[\"train\"]#.train_test_split(0.2)\n",
    "dataset_summary = dataset_summary.rename_column(\"input\", \"log\")\n",
    "dataset_summary = dataset_summary.map(\n",
    "    preprocess_log_batch_hof(),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6d6cee6e-b885-4d0d-a699-7a198dcec617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = dataset_summary.to_pandas()\n",
    "df_summary[\"unilog_vectors\"] = df_summary[\"text\"].map(lambda text: vectorize_using_unilog(text, unilog_bert_model, log_tokenizer_w_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "add5c5dd-6d73-4be8-8acb-67275c30bfc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary[\"fasttext_vectors\"] = df_summary[\"log\"].map(vectorize_log(fasttext_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808ec11-f650-4be9-abec-753744b4fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb6d5ffc-8468-441a-8d5b-d05a50ba5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ref: https://necromuralist.github.io/Neurotic-Networking/posts/nlp/machine-translation-k-nearest-neighbors/index.html\n",
    "\n",
    "def cosine_similarity(vector_1: np.ndarray, vector_2: np.ndarray) -> float:\n",
    "    \"\"\"Calculates the similarity between two vectors\n",
    "\n",
    "    Args:\n",
    "     vector_1: array to compare\n",
    "     vector_2: array to compare to vector_1\n",
    "\n",
    "    Returns:\n",
    "     cosine similarity between the two vectors\n",
    "    \"\"\"\n",
    "    return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1) *\n",
    "                                          np.linalg.norm(vector_2))\n",
    "\n",
    "#fasttextでベクトル化したログデータのうち,k近傍のベクトルを得ている\n",
    "#なお, 論文(https://arxiv.org/abs/2305.15778)ではユークリッド距離と時系列を考慮しているが, 単純化のためここでは単なるコサイン類似度を利用\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    # cosine_similarities = [cosine_similarity(v, row) for row in candidates]\n",
    "\n",
    "    # for each candidate vector...\n",
    "    #for row in candidates:\n",
    "    #    # get the cosine similarity\n",
    "    #    cos_similarity = cosine_similarity(v, row)\n",
    "    #\n",
    "    #    # append the similarity to the list\n",
    "    #    similarity_l.append(cos_similarity)\n",
    "\n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    # sorted_ids = numpy.argsort(similarity_l)\n",
    "\n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    # k_idx = sorted_ids[-k:]\n",
    "    ### END CODE HERE ###\n",
    "    return np.argsort([cosine_similarity(v, row) for row in candidates])[-k:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6a601b5e-2edc-4b5e-8b84-7c9c42b6f66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#プロンプトの内容をGPTに入力し, 出力を得る関数\n",
    "def ask_GPT(input_log, make_prompt, max_tokens=64):\n",
    "    prompt = make_prompt(input_log)\n",
    "    return GPT(prompt, max_tokens=max_tokens)\n",
    "    \n",
    "#プロンプト生成(最もベーシック, ただのfew-shot)\n",
    "def make_prompt_1(input_log, num_samples=3):\n",
    "    prompt = \"\"\"Could you briefly summarize a log message like the following examples? Note: Please do NOT include any numbers and special characters in your answer.\"\"\"\n",
    "\n",
    "    samples = df_summary.sample(num_samples)\n",
    "\n",
    "    i = 0\n",
    "    for log, summary in zip(samples[\"log\"], samples[\"summary\"]):\n",
    "        prompt += f\"\\nQ{i + 1}: {log}\\nA{i + 1}: {summary}\\n\"\n",
    "        i += 1\n",
    "        \n",
    "    prompt += f\"\\nQ{i + 1}:{input_log}\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "#論文(https://arxiv.org/abs/2305.15778)の4.2.2に対応, まずは軽くログの内容を要約\n",
    "def make_summarization_prompt(input_log):\n",
    "    prompt = f\"\"\"\n",
    "        {input_log}\n",
    "        Please summarize the above input. Please note that the above input is incident diagnostic information. The summary results should be about 120 words, no more than 140 words, and should cover important information as much as possible. Just return the summary without any additional output.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "#chatGPTからプロンプト(user_query)対する答えを得る関数\n",
    "def GPT(user_query, model_engine=\"text-davinci-003\", max_tokens=64):\n",
    "    '''\n",
    "    This function uses the OpenAI API to generate a response to the given\n",
    "    user_query using the ChatGPT model\n",
    "    '''\n",
    "    # Use the OpenAI API to generate a response\n",
    "    completion = openai.Completion.create(\n",
    "          engine = model_engine,\n",
    "          prompt = user_query,\n",
    "          max_tokens = max_tokens,\n",
    "          n = 1,\n",
    "          temperature = 1.0,\n",
    "    )\n",
    "    response = completion.choices[0].text\n",
    "    return response\n",
    "\n",
    "#論文(https://arxiv.org/abs/2305.15778)の4.2.4に該当あるように、まずは\n",
    "def make_prompt_ms(model, log_vectors, is_unilog=False, tokenizer=None, verbose=False):\n",
    "    def make_prompt_ms_helper(input_log, num_samples=5):\n",
    "        #論文(https://arxiv.org/abs/2305.15778)の4.2.2の軽い要約を入手\n",
    "        input_summary = ask_GPT(input_log, make_summarization_prompt, max_tokens=128)\n",
    "\n",
    "        prompt = f\"\"\"Context: The following description shows the error\n",
    "            log information of an incident. Please select the\n",
    "            incident information that is most likely to have the\n",
    "            same root cause and give your explanation (just\n",
    "            give one answer). \n",
    "            Input: {input_summary}\n",
    "            Options:\n",
    "    \"\"\"\n",
    "        \n",
    "        #If not, please select the first item “Unseen incident”.\n",
    "            #A: Unseen incident.\n",
    "        alphabet = list(string.ascii_uppercase)\n",
    "        \n",
    "        # 論文(https://arxiv.org/abs/2305.15778)の4.2.3に該当, 現在のログと近い他の例を用意して, 要約結果の選択肢を与える\n",
    "        if is_unilog and tokenizer is not None:\n",
    "            input_vec = vectorize_using_unilog(input_log, model, tokenizer)\n",
    "        else:\n",
    "            input_vec = vectorize_log(model)(input_log)\n",
    "            \n",
    "        neibors = nearest_neighbor(input_vec, log_vectors, k=num_samples)\n",
    "        samples = df_summary.iloc[neibors, :]\n",
    "        \n",
    "        i = 0\n",
    "        summary_set = set()\n",
    "        for summary in samples[\"summary\"]:\n",
    "            if summary not in summary_set:\n",
    "                summary_set.add(summary)\n",
    "                option = alphabet[i]\n",
    "                prompt += f\"{option}: {summary}\\n\"\n",
    "                i += 1\n",
    "        if verbose:\n",
    "            print(\"-\"*40, \"Prompt\", \"-\"*40)\n",
    "            print(prompt)\n",
    "            print(\"-\"*100)\n",
    "\n",
    "        return prompt\n",
    "    return make_prompt_ms_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4abeaf38-92a3-4846-be8e-5cf210550701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rougeを計算する関数\n",
    "#GPTの生成結果は余計な情報もかなり含まれるので, 正規表現で出力部分のみを取り出している\n",
    "#もし正規表現にマッチするものが無かったら答えだけで比較\n",
    "def calculate_rouge(df_result):\n",
    "    \"\"\"\n",
    "    A fuction to calculate rouge score between the ground-truth summaries and GPT-generated ones. \n",
    "    It finally\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f_scores = []\n",
    "    for ans, summary in zip(df_result[\"answers\"], df_result[\"summary\"]):\n",
    "        m = re.search(r\"(?<=Answer: [A-Z]:).*(?=[;\\n])\", ans)\n",
    "        if m is None:\n",
    "            m = re.search(r\"(?<=[A-Z]:).*(?=\\n)\", ans)\n",
    "\n",
    "        ans = m.group(0) if m is not None else ans\n",
    "        rouge_score = rouge.get_scores(ans, summary)[0]\n",
    "        recalls.append(rouge_score[\"rouge-1\"][\"r\"])\n",
    "        precisions.append(rouge_score[\"rouge-1\"][\"p\"])\n",
    "        f_scores.append(rouge_score[\"rouge-1\"][\"f\"])\n",
    "    avg_precision = sum(precisions)/len(df_result)\n",
    "    avg_recall = sum(recalls)/len(df_result)\n",
    "    avg_f_score = sum(f_scores)/len(df_result)\n",
    "    print(f\"Average precision {avg_precision}, recall {avg_recall}, f1 {avg_f_score}\")\n",
    "    return avg_precision, avg_recall, avg_f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2d569b41-8682-406b-b747-d6412d3665b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_current_log(idx):\n",
    "    for column in df_summary.columns:\n",
    "        if \"vector\" not in column:\n",
    "            print(f\"{column}: \", df_summary.iloc[idx, :][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0249db38-8a90-480c-b2ac-2e6c29b0ccdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_log_vectors = np.array(df_summary[\"unilog_vectors\"])\n",
    "curr_log_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "88890f54-b813-4dce-a2b1-02b0b2c2711f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_FASTTEXT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d6de0bf1-7433-4db7-8e53-4cb83e504c53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log:  RAS KERNEL INFO ciod : generated 128 core files for program /g/g90/glosli/src/ddcMD/ddcMD1.1.18a/bin/ddcMDbglV\n",
      "summary:  generated core files for program;\n",
      "text:  ras kernel info cio d generated core files for program g g g los li s rc dd cm d dd cm d a bin dd cm d bg lv\n",
      "---------------------------------------- Prompt ----------------------------------------\n",
      "Context: The following description shows the error\n",
      "            log information of an incident. Please select the\n",
      "            incident information that is most likely to have the\n",
      "            same root cause and give your explanation (just\n",
      "            give one answer). \n",
      "            Input: \n",
      "    The incident diagnostic information states that 128 core files have been generated for the program /g/g90/glosli/src/ddcMD/ddcMD1.1.18a/bin/ddcMDbglV. This implies that the program experienced a crash or hardware issue of some kind, but the specifics are unknown. The core files will assist in streamlining the process of debugging and troubleshooting the issue.\n",
      "            Options:\n",
      "    A: Finished task; Got assigned task; Reading broadcast variable; Partition not found;\n",
      "B: Found block; Got assigned task; Finished task; Running task;\n",
      "C: authentication disabled; Starting remoting; Starting remoting; started service; Connecting to driver; BlockManager stopped; driver disconnected;\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Answer: B. Found block; Got assigned task; Finished task; Running task; \n",
      "Explanation: Option B shows a sequence of events, which suggest that the program experienced a crash or hardware issue of some kind, like the incident diagnostic information states. All events in the sequence are related to program execution\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(df_summary))\n",
    "show_current_log(idx)\n",
    "input_log = df_summary.iloc[idx, :][\"log\"]\n",
    "sample_ans = df_summary.iloc[idx, :][\"summary\"]\n",
    "if not IS_FASTTEXT:\n",
    "    ans = ask_GPT(input_log, make_prompt_ms(unilog_bert_model, curr_log_vectors, is_unilog=True, tokenizer=log_tokenizer_w_n, verbose=True))\n",
    "else:\n",
    "    ans = ask_GPT(input_log, make_prompt_ms(fasttext_model, curr_log_vectors, verbose=True))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "c3be3118-3631-46f7-8638-c338235e2248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df_summary = pd.read_csv(\"./logdata/test_summary_0713.csv\", index_col=0)\n",
    "test_df_summary = df_summary.iloc[test_df_summary.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8023ea-561b-476d-b067-08016d2a2c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 715/1000 [1:14:45<28:19,  5.96s/it]"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for i in tqdm(range(len(test_df_summary))):\n",
    "    sample_input = test_df_summary.iloc[i][\"log\"]\n",
    "    ans = ask_GPT(sample_input, make_prompt_ms(unilog_bert_model, curr_log_vectors, is_unilog=True, tokenizer=log_tokenizer_w_n))\n",
    "    answers.append(ans)\n",
    "\n",
    "test_df_summary[\"answers\"] = answers\n",
    "calculate_rouge(test_df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "690ed43b-4e9d-467b-ba70-c4a06ce9f8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d0b4b257-9a47-4c59-ad00-ffc1f2ac3fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision 0.520789428236848, recall 0.6399072331807628, f1 0.541242909419604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Average precision {sum(precisions)/len(test_df_summary)}, recall {sum(recalls)/len(test_df_summary)}, f1 {sum(f_scores)/len(test_df_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "c2bed0a0-1ef7-4f14-bc1d-fc0a8f538e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df_summary = pd.read_csv(\"./logdata/test_summary_0713.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e264aab4-37f9-4dea-98c3-0ba788051f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision 0.520789428236848, recall 0.6399072331807628, f1 0.541242909419604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.520789428236848, 0.6399072331807628, 0.541242909419604)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_rouge(test_df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c5c0d-dfd7-4ad1-a117-02d9d67fcaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
