{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89522b97-3f30-45fb-b631-ac4ba7c1ad77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import wordninja\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, default_data_collator, get_scheduler\n",
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForPreTraining\n",
    "from hf_transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41930375-6dd4-48ba-a2ce-e244bf7ba940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./logdata/unilog_pretrain_preln_on_attentions_0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hf_transformers.src.transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "#Unilog元論文の実験に準拠\n",
    "#https://arxiv.org/pdf/2112.03159.pdf\n",
    "unilogConfig= BertConfig(\n",
    "    is_unilog=True,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    num_attention_heads=4,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    vocab_size=log_tokenizer_w_n.vocab_size,\n",
    "    num_hidden_layers=3,\n",
    ")\n",
    "unilogConfigmodel = BertForSequenceClassification.from_pretrained(\"./logdata/unilog_pretrain_preln_on_attentions_0/\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25fff3ee-d3ba-47e8-995a-0382ee116462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4075, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): UnilogBertSelfOutput(\n",
       "              (logact): LogACT(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "                (silu): SiLU()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): UnilogBertOutput(\n",
       "            (logact): LogACT(\n",
       "              (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (silu): SiLU()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): UnilogBertSelfOutput(\n",
       "              (logact): LogACT(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "                (silu): SiLU()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): UnilogBertOutput(\n",
       "            (logact): LogACT(\n",
       "              (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (silu): SiLU()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): UnilogBertSelfOutput(\n",
       "              (logact): LogACT(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "                (silu): SiLU()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): UnilogBertOutput(\n",
       "            (logact): LogACT(\n",
       "              (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (silu): SiLU()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unilogConfigmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab319d7e-a5e6-495b-906e-c5c511b2cc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/home/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a58cb4d-7fee-460e-8757-37ce1e980f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#論文中の記述(4ページ), \"tokenizer based on English Wikipedia unigram frequencies\"\n",
    "#Huggingfaceのトークナイザで上記に該当するの\n",
    "    #bert-base-uncased**\n",
    "    #roberta-base\n",
    "    #distilbert-base-uncased\n",
    "    #distilbert-base-multilingual-cased\n",
    "#**実験に利用\n",
    "#unigram freqencies?tokenizerのアルゴリズムは結局何なんだろう? Unigram or WordPiece?\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    \"\"\"\n",
    "    A function that tokenizes all logs\n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"log\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718adf9a-c9ad-454f-b93a-4f7e8fba32fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = []\n",
    "#NOTE\n",
    "#ここで記号を区切ってwordninjaで塊除去を行っている\n",
    "#なお, かなり時間がかかるので実行には注意が必要\n",
    "#もう少し賢く書きたかったが, その書き方を思いつく時間とナイーブに実行する時間はどうせ同じぐらいだろう\n",
    "def preprocess_log(text):\n",
    "    text = text.replace('[', \" \")\n",
    "    text = text.replace(\".\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\":\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\";\", \" \")\n",
    "    text = text.replace(\"=\", \" \")\n",
    "    text = text.replace(\"*\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.lower()\n",
    "    text = \" \".join(wordninja.split(text))\n",
    "    remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "    return remove_num(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ef6c61-ae57-42ce-8145-c6f2c5e41244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602297 node-175 unix.hw state_change.unavailable 1074300564 1 Component State Change: Component \\042alt0\\042 is in the unavailable state (HWID=3389)\n",
      "node unix h w state change unavailable component state change component alt is in the unavailable state h wi d\n"
     ]
    }
   ],
   "source": [
    "print(hdfs[100])\n",
    "print(preprocess_log(hdfs[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f6231289-bfdf-458b-8390-fdb54e64762f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #訓練データとテストデータに分ける\n",
    "# TRAIN_TEST_SPLIT_RATE = 0.8\n",
    "# df_log[\"is_train\"] = np.array([np.random.rand() < TRAIN_TEST_SPLIT_RATE for _ in range(len(df_log))])\n",
    "# df_log[~df_log.is_train].to_csv(\"./logdata/test_logs.csv\")\n",
    "# df_log[df_log.is_train].to_csv(\"./logdata/train_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a042a-1c80-45b6-bb9f-1764c3f698f0",
   "metadata": {},
   "source": [
    "# データセットのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84797a5b-2cd6-4873-98ef-db9332f0b2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/csv/default-a5653168cf0fdeb7/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files = \"./logdata/all_logs.csv\", cache_dir=\"/home/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c57e4d-fede-4047-8294-0cf24ae84082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e487cdf5-d339-4e79-bf5b-bbe138baa020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#データセットのトークナイズ\n",
    "# tokenized_datasets = shuffled_train.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b2c8cf-bb99-4de1-ba1f-501b50fd84ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in tqdm(range(0, len(train_dataset), 1000)):\n",
    "        yield train_dataset[i : i + 1000][\"log\"]\n",
    "        \n",
    "def get_training_corpus_without_numbers():\n",
    "    for i in tqdm(range(0, len(train_dataset), 1000)):\n",
    "        yield train_dataset[i : i + 1000][\"log_without_numbers\"]\n",
    "        \n",
    "def get_training_corpus_small():\n",
    "    for i in tqdm(range(0, len(train_dataset_small_samples), 1000)):\n",
    "        yield train_dataset_small_samples[i : i + 1000][\"log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d58bc-4e6e-4c0a-a3ca-0d08f30a3ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# オリジナルのトークナイザからの訓練(上手くいきませんでした)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c580c9ee-8a30-4579-9e65-fb6574fd9ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "# pre_tokenizer.pre_tokenize_str(train_dataset[1000][\"log\"])\n",
    "\n",
    "# tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "# special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# trainer = trainers.WordPieceTrainer(\n",
    "#     vocab_size=5000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    "# )\n",
    "# tokenizer.train_from_iterator(get_training_corpus_small(), trainer=trainer)\n",
    "# tokenizer.save(f\"./log_tokenizer.json\")\n",
    "\n",
    "# encoding = tokenizer.encode(train_dataset[100][\"log\"])\n",
    "# print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ec888-6698-43d2-a376-31b41cbb9b16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERTトークナイザからの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52961da0-f672-4cd9-ad72-8a39ddf728d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# old_tokenizer(train_dataset[1000][\"log\"])\n",
    "# new_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus_without_numbers(), 25000)\n",
    "# new_tokenizer.save_pretrained(f\"./log_tokenizer_from_old_without_numbers_large\")\n",
    "# new_tokenizer(train_dataset[1000][\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b32f37fb-3a3b-47dc-b079-83cbb1db5977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 180\n",
    "log_tokenizer = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_large/\", model_max_length=MODEL_MAX_LENGTH)\n",
    "log_tokenizer_w_n = AutoTokenizer.from_pretrained(\"./tokenizers/log_tokenizer_from_old_without_numbers/\", model_max_length=MODEL_MAX_LENGTH, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eebcdc-dbce-4c0a-ae86-e8a02b60b1b5",
   "metadata": {},
   "source": [
    "# ログから数字除去したコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74485a88-8a9c-43aa-8999-88eec6694214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_numerical(example):\n",
    "#     remove_num = lambda eg : \" \".join([word for word in eg.split(\" \") if not word.isdigit()])\n",
    "#     example[\"log\"] = [remove_num(eg) for eg in example[\"log\"]]\n",
    "#     return example\n",
    "\n",
    "# dataset_without_numbers = dataset.map(remove_numerical, batched=True)\n",
    "# train_log_without_numbers = train_dataset_without_numbers.to_pandas()\n",
    "# train_log_without_numbers.set_index(train_log_without_numbers[\"Unnamed: 0\"])\n",
    "# train_log_without_numbers = train_log_without_numbers.iloc[:, 1:]\n",
    "# ds_without_numbers = dataset[\"train\"].add_column(\"log_without_numbers\", dataset_without_numbers[\"train\"][\"log\"])\n",
    "# df_without_numbers = ds_without_numbers.to_pandas()\n",
    "# df_without_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a5e9c21-81cc-4ab1-b951-b0590b946924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_datasets_from_disk = load_from_disk(\"./logdata/logdata_for_pretraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4f7aa81-5d5f-4dec-b05f-28c5709403d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "#Unilog元論文(https://arxiv.org/pdf/2112.03159.pdf)の表2の連続マスクを作成するための関数\n",
    "#ランダムでマスクするインデックスの前後もマスクするようにした\n",
    "def make_consecutive_mask(mask):\n",
    "    new_mask = copy.deepcopy(mask)\n",
    "    \n",
    "    masked_indexes = np.where(mask)[0]\n",
    "    \n",
    "    for i, idx in enumerate(masked_indexes):\n",
    "        if idx - 1 >= 0 and idx + 1 < len(new_mask):\n",
    "            if i == 0 or abs(idx - masked_indexes[i-1]) > 3:\n",
    "                new_mask[idx - 1] = 1\n",
    "                new_mask[idx + 1] = 1\n",
    "            else:\n",
    "                new_mask[idx] = 0\n",
    "        else:\n",
    "            new_mask[idx] = 0\n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6f6fc75-1dd0-4c0f-8db7-73adc6d2df45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#参考https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt\n",
    "def consecutive_word_masking_data_collator(features, wwm_probability=0.10):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        # CHANGED\n",
    "        # 論文準拠にマスクを連続的にした\n",
    "        mask = make_consecutive_mask(mask)\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = log_tokenizer_w_n.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "    # print(features)\n",
    "    return default_data_collator(features)\n",
    "\n",
    "#バッチ単位でUnilogのワードマスキングを行う関数\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = consecutive_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27bca825-596c-4739-b0c8-eb30e3661975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] info df s data node packet responder received [MASK] [MASK] [MASK] of size from [SEP] [CLS] r [MASK] [MASK] [MASK] j u r m n c j u ras kernel info microseconds spent in the rb s signal handler during calls microseconds [MASK] [MASK] [MASK] time for a single instance of a correctable ddr [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir b lk [SEP] [CLS] info df s fs name system block name system add stored block block map updated [MASK] [MASK] [MASK] b lk size [SEP] [CLS] info df s data node data x ce iver [MASK] [MASK] [MASK] lk s rc de st [SEP] [CLS] info df s fs data set deleting block b lk file m nt had oop df s data current sub dir b lk [SEP] [CLS] r m ne c j u r m ne c j u ras kernel info torus receiver z input pipe error s d cr [MASK] [MASK] [MASK] and corrected [SEP] [CLS]'\n",
      "\n",
      "'>>> info df s fs name system block name system delete [MASK] [MASK] is added to invalid set of [SEP] [CLS] info df s data node packet responder received block b lk of size from [SEP] [CLS] info rm communicator al locator org apache had oop map [MASK] [MASK] app rm rm container al locator re calculating schedule headroom memory v cores [SEP] [CLS] info df s fs name system block name system delete b lk is added to invalid set [MASK] [SEP] [CLS] [MASK] [MASK] s data node packet responder received block b lk of size from [SEP] [CLS] r m nf c j u r m nf c j u ras kernel info [MASK] [MASK] c c [SEP] [CLS] info df s fs data set deleting [MASK] [MASK] [MASK] file m nt had oop df s data current sub dir b lk [SEP] [CLS] r m nf c j [MASK] [MASK] [MASK] nf c j u ras kernel info [MASK] [MASK] [MASK] [SEP] [CLS] null null ras m mcs error ido proxy db hit assert condition assert expression source'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets_from_disk[i] for i in range(2)]\n",
    "batch = consecutive_word_masking_data_collator(samples, wwm_probability=0.05)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {log_tokenizer_w_n.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996220c4-75c4-4f23-a7c1-7aa5725b0f68",
   "metadata": {},
   "source": [
    "# 訓練用のパラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7ecb756-cc05-40a7-bd61-123cb312e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets_from_disk.train_test_split(test_size=0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccaf928b-60b6-4ee6-aedf-6ab7f985846a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"./logdata/logdata_for_pretraining_train/\")\n",
    "eval_dataset = load_from_disk(\"./logdata/logdata_for_pretraining_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20bec0a1-33da-4906-a7cd-8919148686bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=consecutive_word_masking_data_collator\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a6fb33e-9440-48e4-a02e-7301e1a792a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"is_only_mlm\": false,\n",
       "  \"is_unilog\": true,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4075\n",
       "}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unilog元論文の実験に準拠\n",
    "#https://arxiv.org/pdf/2112.03159.pdf\n",
    "unilogConfig= BertConfig(\n",
    "    is_unilog=True,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    num_attention_heads=4,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    vocab_size=log_tokenizer_w_n.vocab_size,\n",
    "    num_hidden_layers=3\n",
    ")\n",
    "unilogConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d61c0f7-c730-4c61-a2af-4ddcabd1f274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"is_only_mlm\": true,\n",
       "  \"is_unilog\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4075\n",
       "}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertConfig= BertConfig(\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    num_attention_heads=4,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    vocab_size=log_tokenizer_w_n.vocab_size,\n",
    "    num_hidden_layers=12,\n",
    "    is_only_mlm=True\n",
    ")\n",
    "bertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54771836-9a6f-456d-98af-d111c46472cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "model = BertForPreTraining(config=bertConfig).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "num_train_steps = 2**16 / (len(lm_datasets[\"train\"]) // batch_size)\n",
    "\n",
    "num_train_epochs = int(np.ceil(num_train_steps))\n",
    "decayRate = 0.96\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f7ee756-0e87-4b90-a179-585e3f147865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10687\n"
     ]
    }
   ],
   "source": [
    "print(num_train_epochs)\n",
    "output_dir = \"./logdata/bert_log_pretrain_12_layers\"\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50f905-ba56-4efd-8a09-dc516dfca0f9",
   "metadata": {},
   "source": [
    "# 事前学習開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fddeae64-b3eb-479c-8b34-fa131a2e49e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_train_epochs))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        pbar.set_description(f\">>> Loss {loss}\")\n",
    "        pbar.update(1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    # try:\n",
    "    perplexity = math.exp(torch.mean(losses))\n",
    "    # except OverflowError:\n",
    "    #     perplexity = float(\"inf\")\n",
    "    progress_bar.update(1)\n",
    "    progress_bar.write(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    # Save and upload\n",
    "    model.save_pretrained(output_dir + f\"_{epoch}\")\n",
    "    # if accelerator.is_main_process:\n",
    "        # tokenizer.save_pretrained(output_dir)\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ec8d79f-c31d-41c2-98df-e95efa464eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/7 [00:33<?, ?it/s]\u001b[A\u001b[A\n",
      "                                     \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "  0%|          | 0/3 [00:52<?, ?it/s]                                         \n",
      "\n",
      "  0%|          | 0/7 [00:52<?, ?it/s]\u001b[A\u001b[A\n",
      " 33%|███▎      | 1/3 [00:52<01:44, 52.20s/it]25/10687 [01:25<34:05,  5.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 6: Perplexity: 313.2866145731586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 33%|███▎      | 1/3 [01:44<01:44, 52.20s/it]                                 \n",
      "\n",
      "  0%|          | 0/7 [01:44<?, ?it/s]\u001b[A\u001b[A\n",
      " 67%|██████▋   | 2/3 [01:44<00:52, 52.39s/it]25/10687 [02:18<34:05,  5.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 5: Perplexity: 313.2866145731586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 67%|██████▋   | 2/3 [02:36<00:52, 52.39s/it]                                 \n",
      "\n",
      "  0%|          | 0/7 [02:36<?, ?it/s]\u001b[A\u001b[A\n",
      "100%|██████████| 3/3 [02:36<00:00, 52.32s/it]25/10687 [03:10<34:05,  5.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 4: Perplexity: 313.2866145731586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "progress_bar = tqdm(range(num_train_epochs))\n",
    "for epoch in tqdm(range(6,3,-1)):\n",
    "    model6 = BertForPreTraining.from_pretrained(f\"./logdata/bert_log_pretrain_{epoch}/\").to(device)\n",
    "    gc.collect()\n",
    "    model6.eval()\n",
    "    losses = []\n",
    "\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model6(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.repeat(batch_size))\n",
    "        # pbar.update(1)\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    # try:\n",
    "    perplexity = math.exp(torch.mean(losses))\n",
    "    # except OverflowError:\n",
    "    #     perplexity = float(\"inf\")\n",
    "\n",
    "    progress_bar.write(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f956e6-244d-4e72-9412-d158cd6c001a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
